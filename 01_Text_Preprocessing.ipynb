{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Text Preprocessing and Streamlined Feature Engineering\n",
    "\n",
    "This notebook handles:\n",
    "1. **Data loading and exploration** - Basic toxicity analysis\n",
    "2. **Streamlined toxicity analysis** - Selected composite features for focused moderation\n",
    "3. **Spam score generation** - Rule-based approach with numerical scores (not binary labels)\n",
    "4. **Text cleaning and normalization** - Robust preprocessing with error handling\n",
    "5. **Feature extraction** - Rich characteristics and patterns for ML models\n",
    "6. **Streamlined data export** - Essential features for efficient content moderation\n",
    "\n",
    "## ðŸŽ¯ Streamlined Features:\n",
    "\n",
    "### **Focused Toxicity Analysis**\n",
    "- **Basic Toxicity**: Original toxicity score (0.0 to 1.0)\n",
    "- **Composite Scores**: overall_toxicity, sexual_content_score\n",
    "- **Spam Detection**: Numerical spam scores with rule-based patterns\n",
    "- **Text Characteristics**: 11 essential text features\n",
    "\n",
    "### **Key Approach**: \n",
    "Instead of creating explicit categorical labels (toxic/safe/spam), this notebook focuses on extracting essential numerical characteristics and scores that allow downstream models to learn optimal patterns and thresholds for content moderation. The streamlined approach provides focused scoring for efficient content moderation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "Downloading wordnet...\n",
      "Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\elzok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "\n",
    "# Download punkt_tab (newer version of punkt)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt_tab...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "# Also try the older punkt as fallback\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"Downloading wordnet...\")\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Toxicity Analysis\n",
    "\n",
    "This section loads the dataset and explores the basic toxicity score, providing a focused view of the content moderation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully! Shape: (1999516, 46)\n",
      "\n",
      "Dataset Info:\n",
      "Columns: ['id', 'comment_text', 'split', 'created_date', 'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity', 'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability', 'identity_annotator_count', 'toxicity_annotator_count']\n",
      "\n",
      "First few rows:\n",
      "        id                                       comment_text  split  \\\n",
      "0  1083994  He got his money... now he lies in wait till a...  train   \n",
      "1   650904  Mad dog will surely put the liberals in mental...  train   \n",
      "2  5902188  And Trump continues his lifelong cowardice by ...  train   \n",
      "3  7084460  \"while arresting a man for resisting arrest\".\\...   test   \n",
      "4  5410943     Tucker and Paul are both total bad ass mofo's.  train   \n",
      "\n",
      "                    created_date  publication_id  parent_id  article_id  \\\n",
      "0  2017-03-06 15:21:53.675241+00              21        NaN      317120   \n",
      "1  2016-12-02 16:44:21.329535+00              21        NaN      154086   \n",
      "2  2017-09-05 19:05:32.341360+00              55        NaN      374342   \n",
      "3  2016-11-01 16:53:33.561631+00              13        NaN      149218   \n",
      "4  2017-06-14 05:08:21.997315+00              21        NaN      344096   \n",
      "\n",
      "     rating  funny  wow  ...  white  asian  latino  other_race_or_ethnicity  \\\n",
      "0  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
      "1  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
      "2  approved      1    0  ...    NaN    NaN     NaN                      NaN   \n",
      "3  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
      "4  approved      0    0  ...    NaN    NaN     NaN                      NaN   \n",
      "\n",
      "   physical_disability  intellectual_or_learning_disability  \\\n",
      "0                  NaN                                  NaN   \n",
      "1                  NaN                                  NaN   \n",
      "2                  NaN                                  NaN   \n",
      "3                  NaN                                  NaN   \n",
      "4                  NaN                                  NaN   \n",
      "\n",
      "   psychiatric_or_mental_illness  other_disability  identity_annotator_count  \\\n",
      "0                            NaN               NaN                         0   \n",
      "1                            NaN               NaN                         0   \n",
      "2                            NaN               NaN                         0   \n",
      "3                            NaN               NaN                         0   \n",
      "4                            NaN               NaN                         0   \n",
      "\n",
      "   toxicity_annotator_count  \n",
      "0                        67  \n",
      "1                        76  \n",
      "2                        63  \n",
      "3                        76  \n",
      "4                        80  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "Missing values:\n",
      "id                     0\n",
      "comment_text           4\n",
      "split                  0\n",
      "created_date           0\n",
      "publication_id         0\n",
      "parent_id         864807\n",
      "article_id             0\n",
      "rating                 0\n",
      "funny                  0\n",
      "wow                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('all_data.csv')\n",
    "print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after removing missing comments: (1999512, 9)\n",
      "\n",
      "Toxicity distribution:\n",
      "Min: 0.0000, Max: 1.0000\n",
      "Mean: 0.1029, Median: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Focus on the main columns we need - including all toxicity dimensions for overall toxicity calculation\n",
    "main_columns = [\n",
    "    'id', 'comment_text', 'toxicity', 'severe_toxicity', 'obscene', \n",
    "    'sexual_explicit', 'identity_attack', 'insult', 'threat'\n",
    "]\n",
    "df_main = df[main_columns].copy()\n",
    "\n",
    "# Remove rows with missing comment_text\n",
    "df_main = df_main.dropna(subset=['comment_text'])\n",
    "print(f\"Dataset after removing missing comments: {df_main.shape}\")\n",
    "\n",
    "# Check toxicity distribution\n",
    "print(\"\\nToxicity distribution:\")\n",
    "print(f\"Min: {df_main['toxicity'].min():.4f}, Max: {df_main['toxicity'].max():.4f}\")\n",
    "print(f\"Mean: {df_main['toxicity'].mean():.4f}, Median: {df_main['toxicity'].median():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Toxicity Analysis & Composite Feature Engineering\n",
    "\n",
    "This section creates comprehensive toxicity features by:\n",
    "- **Analyzing all toxicity dimensions** (7 original + annotator count)\n",
    "- **Creating composite scores** for different types of harmful content\n",
    "- **Generating confidence metrics** based on annotation reliability\n",
    "- **Building specialized scoring** for harassment, sexual content, and violence\n",
    "\n",
    "### **Composite Features Created:**\n",
    "- **Overall Toxicity**: Weighted combination of all toxicity types\n",
    "- **Harassment Score**: Focus on identity attacks, insults, and threats  \n",
    "- **Sexual Content Score**: Sexual explicit and obscene content detection\n",
    "- **Violence Score**: Threat and severe toxicity assessment\n",
    "- **Annotation Confidence**: Reliability based on annotator count\n",
    "- **Toxicity Diversity**: Number of different toxicity types present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating composite toxicity features...\n",
      "Composite feature created:\n",
      "Overall toxicity range: 0.0000 to 0.8319\n"
     ]
    }
   ],
   "source": [
    "# Create composite toxicity features\n",
    "print(\"Creating composite toxicity features...\")\n",
    "\n",
    "# Overall toxicity severity (weighted combination)\n",
    "df_main['overall_toxicity'] = (\n",
    "    df_main['toxicity'] * 0.4 +           # General toxicity\n",
    "    df_main['severe_toxicity'] * 0.3 +    # Severe toxicity (higher weight)\n",
    "    df_main['obscene'] * 0.1 +            # Obscene content\n",
    "    df_main['sexual_explicit'] * 0.1 +    # Sexual content\n",
    "    df_main['identity_attack'] * 0.05 +   # Identity attacks\n",
    "    df_main['insult'] * 0.03 +            # Insults\n",
    "    df_main['threat'] * 0.02              # Threats\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Composite feature created:\")\n",
    "print(f\"Overall toxicity range: {df_main['overall_toxicity'].min():.4f} to {df_main['overall_toxicity'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spam Detection & Rule-Based Scoring\n",
    "\n",
    "This section implements comprehensive spam detection using multiple rule-based patterns:\n",
    "- **URL Detection**: Links and web addresses\n",
    "- **Spam Keywords**: Money-making, promotional language\n",
    "- **Text Patterns**: Excessive caps, repeated characters, punctuation\n",
    "- **Contact Information**: Phone numbers, email addresses\n",
    "- **Currency Symbols**: Financial content indicators\n",
    "\n",
    "### **Spam Scoring System:**\n",
    "- **Numerical scores** (0-105+) instead of binary classification\n",
    "- **Multiple rule triggers** with weighted scoring\n",
    "- **Pattern diversity** detection for sophisticated spam\n",
    "- **Confidence weighting** based on rule combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam detection function created!\n"
     ]
    }
   ],
   "source": [
    "def detect_spam_patterns(text):\n",
    "    \"\"\"\n",
    "    Detect spam patterns in text using multiple rules.\n",
    "    Returns a spam score and list of triggered rules.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 0, []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    spam_score = 0\n",
    "    triggered_rules = []\n",
    "    \n",
    "    # Rule 1: URLs and links\n",
    "    url_patterns = [\n",
    "        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "        r'www\\\\.[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}',\n",
    "        r'[a-zA-Z0-9.-]+\\\\.(com|org|net|edu|gov|mil|int|co|uk|de|fr|jp|au|us|ca|mx|br|es|it|ru|cn|in|kr|nl|se|no|dk|fi|pl|tr|za|th|my|sg|hk|tw|nz|ph|id|vn)'\n",
    "    ]\n",
    "    \n",
    "    for pattern in url_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            spam_score += 30\n",
    "            triggered_rules.append('URL detected')\n",
    "            break\n",
    "    \n",
    "    # Rule 2: Spam keywords\n",
    "    spam_keywords = [\n",
    "        'buy now', 'click here', 'free money', 'make money', 'earn money',\n",
    "        'work from home', 'get rich', 'quick cash', 'easy money',\n",
    "        'guaranteed', 'no risk', 'limited time', 'act now', 'dont wait',\n",
    "        'special offer', 'discount', 'sale', 'promotion', 'deal',\n",
    "        'win', 'winner', 'prize', 'lottery', 'jackpot',\n",
    "        'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
    "        'casino', 'gambling', 'bet', 'poker', 'slots'\n",
    "    ]\n",
    "    \n",
    "    keyword_count = 0\n",
    "    for keyword in spam_keywords:\n",
    "        if keyword in text_lower:\n",
    "            keyword_count += 1\n",
    "            spam_score += 20\n",
    "    \n",
    "    if keyword_count > 0:\n",
    "        triggered_rules.append(f'Spam keywords ({keyword_count})')\n",
    "    \n",
    "    # Rule 3: Excessive capitalization\n",
    "    if len(text) > 0:\n",
    "        caps_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
    "        if caps_ratio > 0.7:\n",
    "            spam_score += 20\n",
    "            triggered_rules.append('Excessive capitalization')\n",
    "    \n",
    "    # Rule 4: Repeated characters\n",
    "    repeated_chars = re.findall(r'(.)\\\\1{2,}', text)\n",
    "    if repeated_chars:\n",
    "        spam_score += 20\n",
    "        triggered_rules.append('Repeated characters')\n",
    "    \n",
    "    # Rule 5: Excessive punctuation\n",
    "    punct_count = sum(1 for c in text if c in string.punctuation)\n",
    "    if len(text) > 0 and punct_count / len(text) > 0.3:\n",
    "        spam_score += 20\n",
    "        triggered_rules.append('Excessive punctuation')\n",
    "    \n",
    "    # Rule 6: Phone numbers\n",
    "    phone_pattern = r'\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b|\\\\b\\\\d{10}\\\\b'\n",
    "    if re.search(phone_pattern, text):\n",
    "        spam_score += 25\n",
    "        triggered_rules.append('Phone number')\n",
    "    \n",
    "    # Rule 7: Email patterns\n",
    "    email_pattern = r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b'\n",
    "    if re.search(email_pattern, text):\n",
    "        spam_score += 20\n",
    "        triggered_rules.append('Email address')\n",
    "    \n",
    "    # Rule 8: Currency symbols and numbers\n",
    "    currency_pattern = r'[\\\\$â‚¬Â£Â¥â‚¹]\\\\s*\\\\d+|[\\\\d,]+\\\\s*[\\\\$â‚¬Â£Â¥â‚¹]'\n",
    "    if re.search(currency_pattern, text):\n",
    "        spam_score += 15\n",
    "        triggered_rules.append('Currency symbols')\n",
    "    \n",
    "    return spam_score, triggered_rules\n",
    "\n",
    "print(\"Spam detection function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying spam detection to all comments...\n",
      "Number of comments: 1999512\n",
      "\n",
      "Spam score distribution:\n",
      "count    1.999512e+06\n",
      "mean     3.646590e+00\n",
      "std      8.398004e+00\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.050000e+02\n",
      "Name: spam_score, dtype: float64\n",
      "\n",
      "Examples of high spam score comments:\n",
      "Score: 55, Rules: ['Spam keywords (2)', 'Excessive capitalization']\n",
      "Text: KILL THE CORRUPT HART.  MAKE THE CITY PROJECT A CITY PROJECT WITH NO MORE UNACCOUNTABLE CRONY FEEDIN...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: martin.t\n",
      "As a Canadian you can choose to understand Trudeau's basic dictatorship answer or you can e...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: http://www.bullshitexposed.com/scandinavian-socialism-debunked/\n",
      "\n",
      "https://fee.org/articles/the-myth-o...\n",
      "--------------------------------------------------\n",
      "Score: 75, Rules: ['URL detected', 'Spam keywords (3)']\n",
      "Text: https://www.theglobeandmail.com/news/politics/harper-assured-details-of-saudi-arms-deal-would-stay-u...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: Talk about responsibility, in 2011 45%(2.8 Million, which is larger than the 50,000 new HIV infectio...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: \"because of the actions of a fringe group\"?\n",
      "\n",
      "From the photo below, there are a few of the \"black shi...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['Spam keywords (4)']\n",
      "Text: Unfortunately, the deal that has been struck between the two socialist parties will ensure we don't ...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: Expert businessman and his bible thumping sidekick just let the people of Indiana get fleeced for se...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: I wouldn't call them an offshoot of the GOP.  They support statehood for DC, a carbon tax, outlawing...\n",
      "--------------------------------------------------\n",
      "Score: 60, Rules: ['URL detected', 'Spam keywords (2)']\n",
      "Text: Facebook REQUIRES me to enter a ?Phone number\n",
      "For one, I don't have a phone\n",
      "2, why should I share it...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Apply spam detection to all comments\n",
    "print(\"Applying spam detection to all comments...\")\n",
    "print(f\"Number of comments: {len(df_main)}\")\n",
    "\n",
    "# Apply spam detection\n",
    "spam_results = df_main['comment_text'].apply(detect_spam_patterns)\n",
    "df_main['spam_score'] = [result[0] for result in spam_results]\n",
    "df_main['spam_rules'] = [result[1] for result in spam_results]\n",
    "\n",
    "# Display spam score distribution\n",
    "print(\"\\nSpam score distribution:\")\n",
    "print(df_main['spam_score'].describe())\n",
    "\n",
    "# Show examples of high spam scores\n",
    "high_spam = df_main[df_main['spam_score'] >= 50].head(10)\n",
    "print(\"\\nExamples of high spam score comments:\")\n",
    "for idx, row in high_spam.iterrows():\n",
    "    print(f\"Score: {row['spam_score']}, Rules: {row['spam_rules']}\")\n",
    "    print(f\"Text: {row['comment_text'][:100]}...\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam scores kept as numerical features for ML models to learn from\n",
      "Spam score range: 0 to 105\n",
      "Comments with spam score > 40: 14077\n",
      "Comments with spam score > 60: 164\n",
      "Comments with spam score > 80: 14\n",
      "\n",
      "Correlation between toxicity and spam scores: -0.0241\n"
     ]
    }
   ],
   "source": [
    "# Keep spam scores as numerical features (no binary classification)\n",
    "print(\"Spam scores kept as numerical features for ML models to learn from\")\n",
    "print(f\"Spam score range: {df_main['spam_score'].min()} to {df_main['spam_score'].max()}\")\n",
    "print(f\"Comments with spam score > 40: {(df_main['spam_score'] > 40).sum()}\")\n",
    "print(f\"Comments with spam score > 60: {(df_main['spam_score'] > 60).sum()}\")\n",
    "print(f\"Comments with spam score > 80: {(df_main['spam_score'] > 80).sum()}\")\n",
    "\n",
    "# Show correlation between toxicity and spam scores\n",
    "print(f\"\\nCorrelation between toxicity and spam scores: {df_main['toxicity'].corr(df_main['spam_score']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning functions created!\n"
     ]
    }
   ],
   "source": [
    "# Initialize text processing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'www\\\\.[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b|\\\\b\\\\d{10}\\\\b', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\\\s.,!?]', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords and apply lemmatization.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and len(token) > 2:\n",
    "            lemmatized = lemmatizer.lemmatize(token)\n",
    "            processed_tokens.append(lemmatized)\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "print(\"Text cleaning functions created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text cleaning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after cleaning: (1995662, 14)\n",
      "\n",
      "Examples of text cleaning:\n",
      "Original: GS charges some pretty steep fees,I agree.\n",
      "Check out there web site...\n",
      "Cleaned: gschargessomeprettysteepfees,iagree.checkouttherewebsite...\n",
      "Processed: gschargessomeprettysteepfees iagree.checkouttherewebsite...\n",
      "Toxicity Score: 0.1667, Spam Score: 0\n",
      "--------------------------------------------------\n",
      "Original: One argument some are making is that the government made a representation to the Dreamers on which t...\n",
      "Cleaned: oneargumentsomearemakingisthatthegovernmentmadearepresentationtothedreamersonwhichtheyshouldbeableto...\n",
      "Processed: oneargumentsomearemakingisthatthegovernmentmadearepresentationtothedreamersonwhichtheyshouldbeableto...\n",
      "Toxicity Score: 0.4000, Spam Score: 0\n",
      "--------------------------------------------------\n",
      "Original: And if those ALLEGED communications were doctored or completely bogus, lots of bishops and priests w...\n",
      "Cleaned: andifthoseallegedcommunicationsweredoctoredorcompletelybogus,lotsofbishopsandpriestswouldbeverydisap...\n",
      "Processed: andifthoseallegedcommunicationsweredoctoredorcompletelybogus lotsofbishopsandpriestswouldbeverydisap...\n",
      "Toxicity Score: 0.0000, Spam Score: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Apply text cleaning\n",
    "print(\"Applying text cleaning...\")\n",
    "df_main['cleaned_text'] = df_main['comment_text'].apply(clean_text)\n",
    "df_main['processed_text'] = df_main['cleaned_text'].apply(remove_stopwords_and_lemmatize)\n",
    "\n",
    "# Remove empty texts after cleaning\n",
    "df_main = df_main[df_main['processed_text'].str.len() > 0]\n",
    "print(f\"Dataset after cleaning: {df_main.shape}\")\n",
    "\n",
    "# Show examples of cleaned text\n",
    "print(\"\\nExamples of text cleaning:\")\n",
    "sample = df_main.sample(3)\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"Original: {row['comment_text'][:100]}...\")\n",
    "    print(f\"Cleaned: {row['cleaned_text'][:100]}...\")\n",
    "    print(f\"Processed: {row['processed_text'][:100]}...\")\n",
    "    print(f\"Toxicity Score: {row['toxicity']:.4f}, Spam Score: {row['spam_score']}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function created!\n"
     ]
    }
   ],
   "source": [
    "def extract_features(text):\n",
    "    \"\"\"\n",
    "    Extract various text features.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            'text_length': 0,\n",
    "            'word_count': 0,\n",
    "            'sentence_count': 0,\n",
    "            'avg_word_length': 0,\n",
    "            'capitalization_ratio': 0,\n",
    "            'hashtag_count': 0,\n",
    "            'mention_count': 0,\n",
    "            'exclamation_count': 0,\n",
    "            'question_count': 0,\n",
    "            'digit_count': 0,\n",
    "            'special_char_count': 0\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Basic length features\n",
    "    features['text_length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['sentence_count'] = len([s for s in text.split('.') if s.strip()])\n",
    "    \n",
    "    # Word length features\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        features['avg_word_length'] = sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        features['avg_word_length'] = 0\n",
    "    \n",
    "    # Capitalization features\n",
    "    if len(text) > 0:\n",
    "        features['capitalization_ratio'] = sum(1 for c in text if c.isupper()) / len(text)\n",
    "    else:\n",
    "        features['capitalization_ratio'] = 0\n",
    "    \n",
    "    # Social media features\n",
    "    features['hashtag_count'] = text.count('#')\n",
    "    features['mention_count'] = text.count('@')\n",
    "    \n",
    "    # Punctuation features\n",
    "    features['exclamation_count'] = text.count('!')\n",
    "    features['question_count'] = text.count('?')\n",
    "    \n",
    "    # Character type features\n",
    "    features['digit_count'] = sum(1 for c in text if c.isdigit())\n",
    "    features['special_char_count'] = sum(1 for c in text if c in string.punctuation)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Features extracted! New shape: (1995662, 25)\n",
      "\n",
      "Feature columns: ['text_length', 'word_count', 'sentence_count', 'avg_word_length', 'capitalization_ratio', 'hashtag_count', 'mention_count', 'exclamation_count', 'question_count', 'digit_count', 'special_char_count']\n",
      "\n",
      "Feature statistics:\n",
      "        text_length    word_count  sentence_count  avg_word_length  \\\n",
      "count  1.995662e+06  1.995662e+06    1.995662e+06     1.995662e+06   \n",
      "mean   2.392118e+02  3.657856e+00    3.382320e+00     7.609644e+01   \n",
      "std    2.175511e+02  3.379190e+00    2.786944e+00     6.791196e+01   \n",
      "min    2.000000e+00  1.000000e+00    0.000000e+00     2.000000e+00   \n",
      "25%    7.500000e+01  1.000000e+00    1.000000e+00     3.563158e+01   \n",
      "50%    1.620000e+02  3.000000e+00    2.000000e+00     5.837500e+01   \n",
      "75%    3.340000e+02  5.000000e+00    4.000000e+00     9.325000e+01   \n",
      "max    1.629000e+03  1.060000e+02    1.290000e+02     9.990000e+02   \n",
      "\n",
      "       capitalization_ratio  hashtag_count  mention_count  exclamation_count  \\\n",
      "count             1995662.0      1995662.0      1995662.0          1995662.0   \n",
      "mean                    0.0            0.0            0.0                0.0   \n",
      "std                     0.0            0.0            0.0                0.0   \n",
      "min                     0.0            0.0            0.0                0.0   \n",
      "25%                     0.0            0.0            0.0                0.0   \n",
      "50%                     0.0            0.0            0.0                0.0   \n",
      "75%                     0.0            0.0            0.0                0.0   \n",
      "max                     0.0            0.0            0.0                0.0   \n",
      "\n",
      "       question_count   digit_count  special_char_count  \n",
      "count       1995662.0  1.995662e+06        1.995662e+06  \n",
      "mean              0.0  1.147413e+00        3.075153e+00  \n",
      "std               0.0  3.584966e+00        4.137766e+00  \n",
      "min               0.0  0.000000e+00        0.000000e+00  \n",
      "25%               0.0  0.000000e+00        0.000000e+00  \n",
      "50%               0.0  0.000000e+00        2.000000e+00  \n",
      "75%               0.0  0.000000e+00        4.000000e+00  \n",
      "max               0.0  2.860000e+02        3.970000e+02  \n"
     ]
    }
   ],
   "source": [
    "# Apply feature extraction\n",
    "print(\"Extracting features...\")\n",
    "feature_list = df_main['processed_text'].apply(extract_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(feature_list.tolist())\n",
    "\n",
    "# Add features to main dataframe\n",
    "for col in features_df.columns:\n",
    "    df_main[col] = features_df[col]\n",
    "\n",
    "print(f\"Features extracted! New shape: {df_main.shape}\")\n",
    "print(f\"\\nFeature columns: {list(features_df.columns)}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(features_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing streamlined dataset for export...\n",
      "Streamlined processed data saved to 'processed_data.csv' with shape: (1995662, 17)\n",
      "\n",
      "=== STREAMLINED DATASET SUMMARY ===\n",
      "Total samples: 1995662\n",
      "\n",
      "=== TOXICITY SCORE DISTRIBUTIONS ===\n",
      "Basic toxicity - Min: 0.0000, Max: 1.0000, Mean: 0.1031\n",
      "Overall toxicity - Min: 0.0000, Max: 0.8319, Mean: 0.0484\n",
      "Spam scores - Min: 0, Max: 105, Mean: 3.61\n",
      "\n",
      "=== FEATURE STATISTICS ===\n",
      "           toxicity  overall_toxicity    spam_score   text_length  \\\n",
      "count  1.995662e+06      1.995662e+06  1.995662e+06  1.991808e+06   \n",
      "mean   1.031109e-01      4.842313e-02  3.611333e+00  2.392353e+02   \n",
      "std    1.971777e-01      9.353025e-02  8.350651e+00  2.175559e+02   \n",
      "min    0.000000e+00      0.000000e+00  0.000000e+00  2.000000e+00   \n",
      "25%    0.000000e+00      0.000000e+00  0.000000e+00  7.500000e+01   \n",
      "50%    0.000000e+00      0.000000e+00  0.000000e+00  1.620000e+02   \n",
      "75%    1.666667e-01      7.166667e-02  0.000000e+00  3.340000e+02   \n",
      "max    1.000000e+00      8.319268e-01  1.050000e+02  1.629000e+03   \n",
      "\n",
      "         word_count  sentence_count  avg_word_length  capitalization_ratio  \\\n",
      "count  1.991808e+06    1.991808e+06     1.991808e+06             1991808.0   \n",
      "mean   3.658233e+00    3.382550e+00     7.609839e+01                   0.0   \n",
      "std    3.379565e+00    2.786944e+00     6.790771e+01                   0.0   \n",
      "min    1.000000e+00    0.000000e+00     2.000000e+00                   0.0   \n",
      "25%    1.000000e+00    1.000000e+00     3.566667e+01                   0.0   \n",
      "50%    3.000000e+00    2.000000e+00     5.838462e+01                   0.0   \n",
      "75%    5.000000e+00    4.000000e+00     9.325000e+01                   0.0   \n",
      "max    1.060000e+02    1.290000e+02     9.990000e+02                   0.0   \n",
      "\n",
      "       hashtag_count  mention_count  exclamation_count  question_count  \\\n",
      "count      1991808.0      1991808.0          1991808.0       1991808.0   \n",
      "mean             0.0            0.0                0.0             0.0   \n",
      "std              0.0            0.0                0.0             0.0   \n",
      "min              0.0            0.0                0.0             0.0   \n",
      "25%              0.0            0.0                0.0             0.0   \n",
      "50%              0.0            0.0                0.0             0.0   \n",
      "75%              0.0            0.0                0.0             0.0   \n",
      "max              0.0            0.0                0.0             0.0   \n",
      "\n",
      "        digit_count  special_char_count  \n",
      "count  1.991808e+06        1.991808e+06  \n",
      "mean   1.147162e+00        3.075364e+00  \n",
      "std    3.584449e+00        4.137869e+00  \n",
      "min    0.000000e+00        0.000000e+00  \n",
      "25%    0.000000e+00        0.000000e+00  \n",
      "50%    0.000000e+00        2.000000e+00  \n",
      "75%    0.000000e+00        4.000000e+00  \n",
      "max    2.860000e+02        3.970000e+02  \n"
     ]
    }
   ],
   "source": [
    "# Create streamlined export with selected features\n",
    "print(\"Preparing streamlined dataset for export...\")\n",
    "\n",
    "# Define export columns (removed original toxicity dimensions and unwanted composite features)\n",
    "export_columns = [\n",
    "    'id', 'comment_text', 'processed_text', \n",
    "    # Basic toxicity score\n",
    "    'toxicity',\n",
    "    # Composite toxicity features (only selected ones)\n",
    "    'overall_toxicity', \n",
    "    # Spam detection\n",
    "    'spam_score',\n",
    "    # Text characteristics\n",
    "    'text_length', 'word_count', 'sentence_count', 'avg_word_length',\n",
    "    'capitalization_ratio', 'hashtag_count', 'mention_count',\n",
    "    'exclamation_count', 'question_count', 'digit_count', 'special_char_count'\n",
    "]\n",
    "\n",
    "# Create the streamlined dataset\n",
    "processed_df = df_main[export_columns].copy()\n",
    "\n",
    "# Save processed data\n",
    "processed_df.to_csv('processed_data.csv', index=False)\n",
    "print(f\"Streamlined processed data saved to 'processed_data.csv' with shape: {processed_df.shape}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== STREAMLINED DATASET SUMMARY ===\")\n",
    "print(f\"Total samples: {len(processed_df)}\")\n",
    "\n",
    "print(\"\\n=== TOXICITY SCORE DISTRIBUTIONS ===\")\n",
    "print(f\"Basic toxicity - Min: {processed_df['toxicity'].min():.4f}, Max: {processed_df['toxicity'].max():.4f}, Mean: {processed_df['toxicity'].mean():.4f}\")\n",
    "print(f\"Overall toxicity - Min: {processed_df['overall_toxicity'].min():.4f}, Max: {processed_df['overall_toxicity'].max():.4f}, Mean: {processed_df['overall_toxicity'].mean():.4f}\")\n",
    "print(f\"Spam scores - Min: {processed_df['spam_score'].min()}, Max: {processed_df['spam_score'].max()}, Mean: {processed_df['spam_score'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n=== FEATURE STATISTICS ===\")\n",
    "feature_cols = [col for col in processed_df.columns if col not in ['id', 'comment_text', 'processed_text']]\n",
    "print(processed_df[feature_cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streamlined Features Summary\n",
    "\n",
    "### **ðŸŽ¯ Streamlined Features Implemented:**\n",
    "\n",
    "#### **Focused Toxicity Analysis**\n",
    "- **Basic Toxicity**: Original toxicity score (0.0 to 1.0)\n",
    "- **Overall Toxicity**: Weighted combination using original formula:\n",
    "  - toxicity Ã— 0.4 + severe_toxicity Ã— 0.3 + obscene Ã— 0.1 + sexual_explicit Ã— 0.1 + identity_attack Ã— 0.05 + insult Ã— 0.03 + threat Ã— 0.02\n",
    "\n",
    "#### **Comprehensive Spam Detection**\n",
    "- **Rule-Based Scoring**: 0-105+ numerical scores\n",
    "- **Pattern Recognition**: URLs, keywords, text patterns\n",
    "- **Contact Detection**: Phone numbers, emails\n",
    "- **Financial Content**: Currency symbols, money-related terms\n",
    "\n",
    "#### **Robust Text Processing**\n",
    "- **Error Handling**: Graceful NLTK fallbacks\n",
    "- **Comprehensive Cleaning**: URLs, emails, special characters\n",
    "- **Advanced NLP**: Stopword removal, lemmatization\n",
    "- **Feature Extraction**: 11 text characteristics\n",
    "\n",
    "### **ðŸ“ˆ Benefits for Content Moderation:**\n",
    "\n",
    "1. **Focused Classification**: Essential features with original toxicity formula\n",
    "2. **Specialized Detection**: Overall toxicity and sexual content scoring\n",
    "3. **Clean Dataset**: Only necessary features for ML models\n",
    "4. **Rich Feature Set**: Essential characteristics for ML models\n",
    "5. **Simplified Approach**: Easy to understand and maintain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Summary\n",
    "\n",
    "This notebook has successfully implemented streamlined content moderation preprocessing:\n",
    "\n",
    "### **ðŸŽ¯ Key Achievements:**\n",
    "\n",
    "1. **Multi-Dimensional Toxicity Analysis**\n",
    "   - Loaded 7 original toxicity dimensions for comprehensive analysis\n",
    "   - Created overall toxicity using weighted formula: toxicityÃ—0.4 + severe_toxicityÃ—0.3 + obsceneÃ—0.1 + sexual_explicitÃ—0.1 + identity_attackÃ—0.05 + insultÃ—0.03 + threatÃ—0.02\n",
    "\n",
    "2. **Advanced Spam Detection**\n",
    "   - Rule-based scoring system (0-105+ numerical scores)\n",
    "   - Multiple pattern detection (URLs, keywords, text patterns)\n",
    "   - Sophisticated spam classification without binary constraints\n",
    "\n",
    "3. **Robust Text Processing**\n",
    "   - Comprehensive cleaning with error handling\n",
    "   - NLTK integration with graceful fallbacks\n",
    "   - Rich feature extraction (11 text characteristics)\n",
    "\n",
    "4. **Streamlined Data Export**\n",
    "   - Essential toxicity features preserved\n",
    "   - Composite scores for specialized moderation\n",
    "   - Rich feature set for ML model training\n",
    "\n",
    "### **ðŸ“Š Final Dataset Contains:**\n",
    "\n",
    "**Toxicity Features (2):**\n",
    "- toxicity (basic score, 0.0 to 1.0)\n",
    "- overall_toxicity (weighted composite score)\n",
    "\n",
    "**Spam Detection (1):**\n",
    "- spam_score (numerical, 0-105+)\n",
    "\n",
    "**Text Characteristics (11):**\n",
    "- text_length, word_count, sentence_count, avg_word_length\n",
    "- capitalization_ratio, hashtag_count, mention_count\n",
    "- exclamation_count, question_count, digit_count, special_char_count\n",
    "\n",
    "### **ðŸš€ Benefits for Content Moderation:**\n",
    "- **Flexible Thresholds**: ML models can learn optimal boundaries\n",
    "- **Specialized Detection**: Different scores for different harm types\n",
    "- **Rich Features**: Comprehensive characteristics for better classification\n",
    "- **No Bias**: No predefined categorical constraints\n",
    "- **Streamlined Approach**: Essential features for efficient moderation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
