{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Rule-Based Content Moderation Filter\n",
        "\n",
        "This notebook implements a comprehensive rule-based filtering system for content moderation that works with the processed data from the text preprocessing phase.\n",
        "\n",
        "## ğŸ¯ Features:\n",
        "1. **Toxicity Detection**: Rule-based toxicity classification using multiple criteria\n",
        "2. **Spam Detection**: Advanced spam pattern recognition with detailed explanations\n",
        "3. **Safe Content Identification**: Clean content classification\n",
        "4. **Comprehensive Testing**: Validation using processed_data.csv\n",
        "5. **Decision Explanations**: Detailed reasoning for each classification decision\n",
        "\n",
        "## ğŸ“Š Integration with Processed Data:\n",
        "- Uses features from processed_data.csv: toxicity, overall_toxicity, spam_score, text characteristics\n",
        "- Combines rule-based logic with pre-computed toxicity scores\n",
        "- Provides detailed explanations for classification decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Keyword Blacklist Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Offensive keyword detection function created!\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive offensive keywords list\n",
        "OFFENSIVE_KEYWORDS = [\n",
        "    # Profanity and slurs\n",
        "    'fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt', 'piss',\n",
        "    'crap', 'hell', 'dick', 'pussy', 'cock', 'whore', 'slut', 'fag',\n",
        "    'nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead',\n",
        "    'retard', 'retarded', 'moron', 'idiot', 'stupid', 'dumb', 'fool',\n",
        "    'bullshit', 'crap', 'sucks', 'terrible', 'awful', 'horrible',\n",
        "    \n",
        "    # Extended racism and ethnic slurs\n",
        "    'nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead',\n",
        "    'gook', 'jap', 'chink', 'slant', 'yellow', 'redskin', 'savage',\n",
        "    'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy',\n",
        "    'house nigger', 'field nigger', 'oreo', 'coconut', 'banana',\n",
        "    'wetback', 'beaner', 'spic', 'greaser', 'taco', 'burrito',\n",
        "    'sand nigger', 'camel jockey', 'raghead', 'towelhead', 'haji',\n",
        "    'chink', 'gook', 'slant eye', 'yellow', 'rice eater', 'dog eater',\n",
        "    'kike', 'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl',\n",
        "    'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig',\n",
        "    'gypsy', 'gyp', 'pikey', 'tinker', 'traveller',\n",
        "    \n",
        "    # Violence and threats\n",
        "    'kill', 'murder', 'death', 'die', 'suicide', 'bomb', 'explode',\n",
        "    'shoot', 'gun', 'weapon', 'knife', 'stab', 'beat', 'hit', 'punch',\n",
        "    'threat', 'threaten', 'harm', 'hurt', 'destroy', 'annihilate',\n",
        "    \n",
        "    # Hate speech\n",
        "    'hate', 'hater', 'racist', 'sexist', 'homophobic', 'transphobic',\n",
        "    'nazi', 'hitler', 'white power', 'black power', 'supremacist',\n",
        "    'genocide', 'ethnic cleansing', 'apartheid', 'segregation',\n",
        "    \n",
        "    # Sexual content\n",
        "    'porn', 'pornography', 'sex', 'sexual', 'nude', 'naked', 'nude',\n",
        "    'breast', 'boob', 'tit', 'vagina', 'penis', 'dick', 'pussy',\n",
        "    'rape', 'raping', 'molest', 'pedophile', 'pedo', 'incest',\n",
        "    \n",
        "    # Drugs and alcohol\n",
        "    'cocaine', 'heroin', 'marijuana', 'weed', 'cannabis', 'crack',\n",
        "    'meth', 'methamphetamine', 'ecstasy', 'lsd', 'acid', 'mushroom',\n",
        "    'alcohol', 'drunk', 'drinking', 'beer', 'wine', 'vodka',\n",
        "    \n",
        "    # Spam and scams\n",
        "    'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
        "    'casino', 'gambling', 'bet', 'poker', 'slots', 'lottery',\n",
        "    'free money', 'make money', 'earn money', 'quick cash',\n",
        "    'work from home', 'get rich', 'guaranteed', 'no risk',\n",
        "    'click here', 'buy now', 'special offer', 'limited time'\n",
        "]\n",
        "\n",
        "# Offensive abbreviations and internet slang\n",
        "OFFENSIVE_ABBREVIATIONS = {\n",
        "    'wtf': 'what the fuck',\n",
        "    'stfu': 'shut the fuck up',\n",
        "    'gtfo': 'get the fuck out',\n",
        "    'fml': 'fuck my life',\n",
        "    'omfg': 'oh my fucking god',\n",
        "    'lmao': 'laughing my ass off',\n",
        "    'rofl': 'rolling on floor laughing',\n",
        "    'stfu': 'shut the fuck up',\n",
        "    'gtfo': 'get the fuck out',\n",
        "    'fml': 'fuck my life',\n",
        "    'omfg': 'oh my fucking god',\n",
        "    'lmao': 'laughing my ass off',\n",
        "    'rofl': 'rolling on floor laughing',\n",
        "    'af': 'as fuck',\n",
        "    'btw': 'by the way',\n",
        "    'fyi': 'for your information',\n",
        "    'tbh': 'to be honest',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my humble opinion',\n",
        "    'smh': 'shaking my head',\n",
        "    'tbh': 'to be honest',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my humble opinion',\n",
        "    'smh': 'shaking my head',\n",
        "    'fyi': 'for your information',\n",
        "    'btw': 'by the way',\n",
        "    'af': 'as fuck'\n",
        "}\n",
        "\n",
        "def check_offensive_keywords(text):\n",
        "    \"\"\"\n",
        "    Check if text contains any offensive keywords and abbreviations.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains 'is_offensive', 'offensive_words', 'offense_count', and 'offense_score'\n",
        "    \"\"\"\n",
        "    import re  # Import re at the beginning of the function\n",
        "    \n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_offensive': False,\n",
        "            'offensive_words': [],\n",
        "            'offense_count': 0,\n",
        "            'offense_score': 0,\n",
        "            'found_abbreviations': []\n",
        "        }\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    found_words = []\n",
        "    found_abbreviations = []\n",
        "    \n",
        "    # Check for exact matches in offensive keywords\n",
        "    for word in OFFENSIVE_KEYWORDS:\n",
        "        if word in text_lower:\n",
        "            found_words.append(word)\n",
        "    \n",
        "    # Check for partial matches (word boundaries)\n",
        "    for word in OFFENSIVE_KEYWORDS:\n",
        "        pattern = r'\\\\b' + re.escape(word) + r'\\\\b'\n",
        "        if re.search(pattern, text_lower):\n",
        "            if word not in found_words:\n",
        "                found_words.append(word)\n",
        "    \n",
        "    # Check for offensive abbreviations with sentence context\n",
        "    # Split by multiple sentence endings\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for abbrev, full_form in OFFENSIVE_ABBREVIATIONS.items():\n",
        "        if abbrev in text_lower:\n",
        "            # Find the sentence containing the abbreviation\n",
        "            containing_sentence = \"\"\n",
        "            for sentence in sentences:\n",
        "                if abbrev.lower() in sentence.lower():\n",
        "                    containing_sentence = sentence.strip()\n",
        "                    break\n",
        "            \n",
        "            if containing_sentence:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form}) in: '{containing_sentence}'\")\n",
        "            else:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form})\")\n",
        "            found_words.append(abbrev)\n",
        "    \n",
        "    # Calculate offense score based on severity\n",
        "    offense_score = 0\n",
        "    for word in found_words:\n",
        "        if word in ['nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead', 'gook', 'jap', 'slant', 'yellow', 'redskin', 'savage', 'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy', 'house nigger', 'field nigger', 'oreo', 'coconut', 'banana', 'beaner', 'greaser', 'taco', 'burrito', 'sand nigger', 'camel jockey', 'raghead', 'haji', 'slant eye', 'rice eater', 'dog eater', 'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl', 'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig', 'gypsy', 'gyp', 'pikey', 'tinker', 'traveller']:\n",
        "            offense_score += 50  # High severity racial slurs\n",
        "        elif word in ['fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt']:\n",
        "            offense_score += 30  # Medium severity profanity\n",
        "        elif word in ['kill', 'murder', 'death', 'suicide', 'bomb', 'explode']:\n",
        "            offense_score += 40  # High severity violence\n",
        "        elif word in ['hate', 'hater', 'racist', 'sexist', 'homophobic']:\n",
        "            offense_score += 25  # Medium severity hate speech\n",
        "        elif word in ['wtf', 'stfu', 'gtfo', 'fml', 'omfg']:\n",
        "            offense_score += 20  # Offensive abbreviations\n",
        "        else:\n",
        "            offense_score += 10  # Low severity\n",
        "    \n",
        "    return {\n",
        "        'is_offensive': len(found_words) > 0,\n",
        "        'offensive_words': found_words,\n",
        "        'offense_count': len(found_words),\n",
        "        'offense_score': offense_score,\n",
        "        'found_abbreviations': found_abbreviations\n",
        "    }\n",
        "\n",
        "print(\"Offensive keyword detection function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Spam Pattern Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spam pattern detection function created!\n"
          ]
        }
      ],
      "source": [
        "def detect_spam_patterns(text):\n",
        "    \"\"\"\n",
        "    Detect spam patterns in text using regex patterns.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains spam detection results and triggered patterns\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_spam_pattern': False,\n",
        "            'spam_patterns': [],\n",
        "            'spam_score': 0,\n",
        "            'details': {},\n",
        "            'found_urls': [],\n",
        "            'found_emails': [],\n",
        "            'found_phones': []\n",
        "        }\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    spam_patterns = []\n",
        "    spam_score = 0\n",
        "    details = {}\n",
        "    found_urls = []\n",
        "    found_emails = []\n",
        "    found_phones = []\n",
        "    \n",
        "    # URL patterns with detailed extraction\n",
        "    url_patterns = [\n",
        "        (r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'URL detected', 30),\n",
        "        (r'www\\\\.[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}', 'WWW URL detected', 25),\n",
        "        (r'[a-zA-Z0-9.-]+\\\\.(com|org|net|edu|gov|mil|int|co|uk|de|fr|jp|au|us|ca|mx|br|es|it|ru|cn|in|kr|nl|se|no|dk|fi|pl|tr|za|th|my|sg|hk|tw|nz|ph|id|vn)', 'Domain detected', 20)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in url_patterns:\n",
        "        matches = re.findall(pattern, text_lower)\n",
        "        if matches:\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "            found_urls.extend(matches)\n",
        "    \n",
        "    # Email patterns with extraction\n",
        "    email_pattern = r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b'\n",
        "    email_matches = re.findall(email_pattern, text)\n",
        "    if email_matches:\n",
        "        spam_patterns.append('Email address')\n",
        "        spam_score += 20\n",
        "        details['Email address'] = True\n",
        "        found_emails.extend(email_matches)\n",
        "    \n",
        "    # Phone number patterns with extraction\n",
        "    phone_patterns = [\n",
        "        (r'\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b', 'Phone number (US format)', 25),\n",
        "        (r'\\\\b\\\\d{10}\\\\b', 'Phone number (10 digits)', 20),\n",
        "        (r'\\\\+\\\\d{1,3}\\\\s?\\\\d{1,14}', 'International phone number', 30)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in phone_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "            found_phones.extend(matches)\n",
        "    \n",
        "    # Currency and money patterns\n",
        "    currency_patterns = [\n",
        "        (r'[\\\\$â‚¬Â£Â¥â‚¹]\\\\s*\\\\d+', 'Currency symbol with number', 15),\n",
        "        (r'\\\\d+\\\\s*[\\\\$â‚¬Â£Â¥â‚¹]', 'Number with currency symbol', 15),\n",
        "        (r'\\\\d{1,3}(,\\\\d{3})*\\\\s*[\\\\$â‚¬Â£Â¥â‚¹]', 'Formatted currency', 20),\n",
        "        (r'\\\\$\\\\d+', 'Dollar amount', 10)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in currency_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "    \n",
        "    # Promotional keywords with specific extraction\n",
        "    promo_keywords = [\n",
        "        'buy now', 'click here', 'free money', 'make money', 'earn money',\n",
        "        'work from home', 'get rich', 'quick cash', 'easy money',\n",
        "        'guaranteed', 'no risk', 'limited time', 'act now', 'dont wait',\n",
        "        'special offer', 'discount', 'sale', 'promotion', 'deal',\n",
        "        'win', 'winner', 'prize', 'lottery', 'jackpot'\n",
        "    ]\n",
        "    \n",
        "    found_promo_keywords = []\n",
        "    for keyword in promo_keywords:\n",
        "        if keyword in text_lower:\n",
        "            found_promo_keywords.append(keyword)\n",
        "            spam_score += 10\n",
        "    \n",
        "    if found_promo_keywords:\n",
        "        spam_patterns.append(f'Promotional keywords ({len(found_promo_keywords)})')\n",
        "        details['Promotional keywords'] = found_promo_keywords\n",
        "    \n",
        "    # Medical/pharmaceutical keywords\n",
        "    medical_keywords = [\n",
        "        'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
        "        'drug', 'pill', 'tablet', 'capsule', 'dosage'\n",
        "    ]\n",
        "    \n",
        "    medical_count = 0\n",
        "    for keyword in medical_keywords:\n",
        "        if keyword in text_lower:\n",
        "            medical_count += 1\n",
        "            spam_score += 15\n",
        "    \n",
        "    if medical_count > 0:\n",
        "        spam_patterns.append(f'Medical keywords ({medical_count})')\n",
        "        details['Medical keywords'] = medical_count\n",
        "    \n",
        "    # Gambling keywords with specific extraction\n",
        "    gambling_keywords = [\n",
        "        'casino', 'gambling', 'bet', 'poker', 'slots', 'lottery',\n",
        "        'jackpot', 'win', 'winner', 'prize', 'money back'\n",
        "    ]\n",
        "    \n",
        "    found_gambling_keywords = []\n",
        "    for keyword in gambling_keywords:\n",
        "        if keyword in text_lower:\n",
        "            found_gambling_keywords.append(keyword)\n",
        "            spam_score += 12\n",
        "    \n",
        "    if found_gambling_keywords:\n",
        "        spam_patterns.append(f'Gambling keywords ({len(found_gambling_keywords)})')\n",
        "        details['Gambling keywords'] = found_gambling_keywords\n",
        "    \n",
        "    return {\n",
        "        'is_spam_pattern': len(spam_patterns) > 0,\n",
        "        'spam_patterns': spam_patterns,\n",
        "        'spam_score': spam_score,\n",
        "        'details': details,\n",
        "        'found_urls': found_urls,\n",
        "        'found_emails': found_emails,\n",
        "        'found_phones': found_phones\n",
        "    }\n",
        "\n",
        "print(\"Spam pattern detection function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Character Repetition and Capitalization Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character pattern detection function created!\n"
          ]
        }
      ],
      "source": [
        "def check_character_patterns(text):\n",
        "    \"\"\"\n",
        "    Check for excessive capitalization, character repetition, and other suspicious patterns.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains character pattern analysis results\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_suspicious_pattern': False,\n",
        "            'suspicious_patterns': [],\n",
        "            'pattern_score': 0,\n",
        "            'details': {}\n",
        "        }\n",
        "    \n",
        "    suspicious_patterns = []\n",
        "    pattern_score = 0\n",
        "    details = {}\n",
        "    \n",
        "    # Check excessive capitalization\n",
        "    if len(text) > 0:\n",
        "        caps_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
        "        if caps_ratio > 0.7:\n",
        "            suspicious_patterns.append('Excessive capitalization')\n",
        "            pattern_score += 25\n",
        "            details['Capitalization ratio'] = f\"{caps_ratio:.2f}\"\n",
        "        elif caps_ratio > 0.5:\n",
        "            suspicious_patterns.append('High capitalization')\n",
        "            pattern_score += 15\n",
        "            details['Capitalization ratio'] = f\"{caps_ratio:.2f}\"\n",
        "    \n",
        "    # Check repeated characters\n",
        "    repeated_chars = re.findall(r'(.)\\\\1{2,}', text)\n",
        "    if repeated_chars:\n",
        "        suspicious_patterns.append('Repeated characters')\n",
        "        pattern_score += 20\n",
        "        details['Repeated characters'] = repeated_chars[:5]  # Show first 5\n",
        "    \n",
        "    # Check excessive punctuation\n",
        "    if len(text) > 0:\n",
        "        punct_count = sum(1 for c in text if c in string.punctuation)\n",
        "        punct_ratio = punct_count / len(text)\n",
        "        if punct_ratio > 0.3:\n",
        "            suspicious_patterns.append('Excessive punctuation')\n",
        "            pattern_score += 20\n",
        "            details['Punctuation ratio'] = f\"{punct_ratio:.2f}\"\n",
        "        elif punct_ratio > 0.2:\n",
        "            suspicious_patterns.append('High punctuation')\n",
        "            pattern_score += 10\n",
        "            details['Punctuation ratio'] = f\"{punct_ratio:.2f}\"\n",
        "    \n",
        "    # Check for excessive exclamation marks\n",
        "    exclamation_count = text.count('!')\n",
        "    if exclamation_count > 5:\n",
        "        suspicious_patterns.append('Excessive exclamation marks')\n",
        "        pattern_score += 15\n",
        "        details['Exclamation count'] = exclamation_count\n",
        "    elif exclamation_count > 3:\n",
        "        suspicious_patterns.append('High exclamation marks')\n",
        "        pattern_score += 8\n",
        "        details['Exclamation count'] = exclamation_count\n",
        "    \n",
        "    # Check for excessive question marks\n",
        "    question_count = text.count('?')\n",
        "    if question_count > 5:\n",
        "        suspicious_patterns.append('Excessive question marks')\n",
        "        pattern_score += 15\n",
        "        details['Question count'] = question_count\n",
        "    elif question_count > 3:\n",
        "        suspicious_patterns.append('High question marks')\n",
        "        pattern_score += 8\n",
        "        details['Question count'] = question_count\n",
        "    \n",
        "    # Check for excessive numbers\n",
        "    if len(text) > 0:\n",
        "        digit_count = sum(1 for c in text if c.isdigit())\n",
        "        digit_ratio = digit_count / len(text)\n",
        "        if digit_ratio > 0.3:\n",
        "            suspicious_patterns.append('Excessive numbers')\n",
        "            pattern_score += 15\n",
        "            details['Digit ratio'] = f\"{digit_ratio:.2f}\"\n",
        "        elif digit_ratio > 0.2:\n",
        "            suspicious_patterns.append('High numbers')\n",
        "            pattern_score += 8\n",
        "            details['Digit ratio'] = f\"{digit_ratio:.2f}\"\n",
        "    \n",
        "    # Check for excessive special characters\n",
        "    if len(text) > 0:\n",
        "        special_count = sum(1 for c in text if c in string.punctuation)\n",
        "        special_ratio = special_count / len(text)\n",
        "        if special_ratio > 0.4:\n",
        "            suspicious_patterns.append('Excessive special characters')\n",
        "            pattern_score += 20\n",
        "            details['Special char ratio'] = f\"{special_ratio:.2f}\"\n",
        "        elif special_ratio > 0.3:\n",
        "            suspicious_patterns.append('High special characters')\n",
        "            pattern_score += 10\n",
        "            details['Special char ratio'] = f\"{special_ratio:.2f}\"\n",
        "    \n",
        "    # Check for very short or very long text\n",
        "    text_length = len(text)\n",
        "    if text_length < 10:\n",
        "        suspicious_patterns.append('Very short text')\n",
        "        pattern_score += 10\n",
        "        details['Text length'] = text_length\n",
        "    elif text_length > 1000:\n",
        "        suspicious_patterns.append('Very long text')\n",
        "        pattern_score += 5\n",
        "        details['Text length'] = text_length\n",
        "    \n",
        "    # Check for all caps words\n",
        "    words = text.split()\n",
        "    all_caps_words = [word for word in words if word.isupper() and len(word) > 2]\n",
        "    if len(all_caps_words) > 3:\n",
        "        suspicious_patterns.append('Multiple all-caps words')\n",
        "        pattern_score += 15\n",
        "        details['All-caps words'] = all_caps_words[:5]  # Show first 5\n",
        "    elif len(all_caps_words) > 1:\n",
        "        suspicious_patterns.append('Some all-caps words')\n",
        "        pattern_score += 8\n",
        "        details['All-caps words'] = all_caps_words[:3]  # Show first 3\n",
        "    \n",
        "    return {\n",
        "        'is_suspicious_pattern': len(suspicious_patterns) > 0,\n",
        "        'suspicious_patterns': suspicious_patterns,\n",
        "        'pattern_score': pattern_score,\n",
        "        'details': details\n",
        "    }\n",
        "\n",
        "print(\"Character pattern detection function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Master Function - Apply All Rules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Master rule application function created!\n"
          ]
        }
      ],
      "source": [
        "def apply_rules(text):\n",
        "    \"\"\"\n",
        "    Master function that applies all rule-based checks to a text.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        \n",
        "    Returns:\n",
        "        dict: Comprehensive analysis results from all rule-based checks\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_offensive': False,\n",
        "            'is_spam_pattern': False,\n",
        "            'is_suspicious_pattern': False,\n",
        "            'risk_score_modifier': 0,\n",
        "            'offensive_details': {},\n",
        "            'spam_details': {},\n",
        "            'pattern_details': {},\n",
        "            'summary': 'No analysis performed - invalid input'\n",
        "        }\n",
        "    \n",
        "    # Apply all rule-based checks\n",
        "    offensive_result = check_offensive_keywords(text)\n",
        "    spam_result = detect_spam_patterns(text)\n",
        "    pattern_result = check_character_patterns(text)\n",
        "    \n",
        "    # Calculate overall risk score modifier\n",
        "    risk_score_modifier = (\n",
        "        offensive_result['offense_score'] + \n",
        "        spam_result['spam_score'] + \n",
        "        pattern_result['pattern_score']\n",
        "    )\n",
        "    \n",
        "    # Determine if any rules were triggered\n",
        "    any_rules_triggered = (\n",
        "        offensive_result['is_offensive'] or \n",
        "        spam_result['is_spam_pattern'] or \n",
        "        pattern_result['is_suspicious_pattern']\n",
        "    )\n",
        "    \n",
        "    # Create summary\n",
        "    summary_parts = []\n",
        "    if offensive_result['is_offensive']:\n",
        "        summary_parts.append(f\"Offensive content detected ({offensive_result['offense_count']} words)\")\n",
        "    if spam_result['is_spam_pattern']:\n",
        "        summary_parts.append(f\"Spam patterns detected ({len(spam_result['spam_patterns'])} patterns)\")\n",
        "    if pattern_result['is_suspicious_pattern']:\n",
        "        summary_parts.append(f\"Suspicious patterns detected ({len(pattern_result['suspicious_patterns'])} patterns)\")\n",
        "    \n",
        "    if not summary_parts:\n",
        "        summary = \"No rule violations detected\"\n",
        "    else:\n",
        "        summary = \"; \".join(summary_parts)\n",
        "    \n",
        "    return {\n",
        "        'is_offensive': offensive_result['is_offensive'],\n",
        "        'is_spam_pattern': spam_result['is_spam_pattern'],\n",
        "        'is_suspicious_pattern': pattern_result['is_suspicious_pattern'],\n",
        "        'risk_score_modifier': risk_score_modifier,\n",
        "        'offensive_details': {\n",
        "            'offensive_words': offensive_result['offensive_words'],\n",
        "            'offense_count': offensive_result['offense_count'],\n",
        "            'offense_score': offensive_result['offense_score'],\n",
        "            'found_abbreviations': offensive_result['found_abbreviations']\n",
        "        },\n",
        "        'spam_details': {\n",
        "            'spam_patterns': spam_result['spam_patterns'],\n",
        "            'spam_score': spam_result['spam_score'],\n",
        "            'details': spam_result['details'],\n",
        "            'found_urls': spam_result['found_urls'],\n",
        "            'found_emails': spam_result['found_emails'],\n",
        "            'found_phones': spam_result['found_phones']\n",
        "        },\n",
        "        'pattern_details': {\n",
        "            'suspicious_patterns': pattern_result['suspicious_patterns'],\n",
        "            'pattern_score': pattern_result['pattern_score'],\n",
        "            'details': pattern_result['details']\n",
        "        },\n",
        "        'summary': summary\n",
        "    }\n",
        "\n",
        "print(\"Master rule application function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Test with Processed Data\n",
        "\n",
        "This section loads the processed data from the text preprocessing phase and tests our rule-based system with real data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processed data from text preprocessing phase...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed data loaded successfully! Shape: (1995662, 17)\n",
            "ğŸ“Š Available columns: ['id', 'comment_text', 'processed_text', 'toxicity', 'overall_toxicity', 'spam_score', 'text_length', 'word_count', 'sentence_count', 'avg_word_length', 'capitalization_ratio', 'hashtag_count', 'mention_count', 'exclamation_count', 'question_count', 'digit_count', 'special_char_count']\n",
            "\n",
            "ğŸ“ˆ Data Statistics:\n",
            "Total samples: 1,995,662\n",
            "Toxicity range: 0.0000 - 1.0000\n",
            "Overall toxicity range: 0.0000 - 0.8319\n",
            "Spam score range: 0 - 105\n"
          ]
        }
      ],
      "source": [
        "# Load the processed data from text preprocessing\n",
        "print(\"Loading processed data from text preprocessing phase...\")\n",
        "try:\n",
        "    df_processed = pd.read_csv('processed_data.csv')\n",
        "    print(f\"âœ… Processed data loaded successfully! Shape: {df_processed.shape}\")\n",
        "    print(f\"ğŸ“Š Available columns: {list(df_processed.columns)}\")\n",
        "    \n",
        "    # Display basic statistics\n",
        "    print(f\"\\nğŸ“ˆ Data Statistics:\")\n",
        "    print(f\"Total samples: {len(df_processed):,}\")\n",
        "    print(f\"Toxicity range: {df_processed['toxicity'].min():.4f} - {df_processed['toxicity'].max():.4f}\")\n",
        "    print(f\"Overall toxicity range: {df_processed['overall_toxicity'].min():.4f} - {df_processed['overall_toxicity'].max():.4f}\")\n",
        "    print(f\"Spam score range: {df_processed['spam_score'].min()} - {df_processed['spam_score'].max()}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ processed_data.csv not found. Please run the text preprocessing notebook first.\")\n",
        "    df_processed = None\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading processed data: {e}\")\n",
        "    df_processed = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Comprehensive classification function with explanations created!\n"
          ]
        }
      ],
      "source": [
        "def classify_content_with_explanation(row):\n",
        "    \"\"\"\n",
        "    Comprehensive content classification using rule-based logic with detailed explanations.\n",
        "    \n",
        "    Args:\n",
        "        row: Pandas row containing processed data features\n",
        "        \n",
        "    Returns:\n",
        "        dict: Classification result with detailed explanation\n",
        "    \"\"\"\n",
        "    # Extract features\n",
        "    text = row['comment_text']\n",
        "    toxicity = row['toxicity']\n",
        "    overall_toxicity = row['overall_toxicity']\n",
        "    spam_score = row['spam_score']\n",
        "    \n",
        "    # Apply rule-based checks\n",
        "    rule_result = apply_rules(text)\n",
        "    \n",
        "    # Classification logic with explanations\n",
        "    classification = \"safe\"\n",
        "    confidence = 0.0\n",
        "    explanation_parts = []\n",
        "    risk_factors = []\n",
        "    \n",
        "    # 1. Spam Detection (Highest Priority)\n",
        "    if spam_score >= 60 or rule_result['spam_details']['spam_score'] >= 60:\n",
        "        classification = \"spam\"\n",
        "        confidence = 0.9\n",
        "        explanation_parts.append(\"High spam score detected\")\n",
        "        risk_factors.append(f\"Spam score: {spam_score}\")\n",
        "        if rule_result['spam_details']['spam_patterns']:\n",
        "            risk_factors.append(f\"Spam patterns: {', '.join(rule_result['spam_details']['spam_patterns'])}\")\n",
        "        if rule_result['spam_details']['found_urls']:\n",
        "            risk_factors.append(f\"URLs found: {', '.join(rule_result['spam_details']['found_urls'][:3])}\")\n",
        "        if rule_result['spam_details']['found_emails']:\n",
        "            risk_factors.append(f\"Emails found: {', '.join(rule_result['spam_details']['found_emails'][:2])}\")\n",
        "        if rule_result['spam_details']['found_phones']:\n",
        "            risk_factors.append(f\"Phone numbers found: {', '.join(rule_result['spam_details']['found_phones'][:2])}\")\n",
        "        if 'Promotional keywords' in rule_result['spam_details']['details']:\n",
        "            promo_keywords = rule_result['spam_details']['details']['Promotional keywords']\n",
        "            if isinstance(promo_keywords, list):\n",
        "                risk_factors.append(f\"Promotional keywords found: {', '.join(promo_keywords[:3])}\")\n",
        "        if 'Gambling keywords' in rule_result['spam_details']['details']:\n",
        "            gambling_keywords = rule_result['spam_details']['details']['Gambling keywords']\n",
        "            if isinstance(gambling_keywords, list):\n",
        "                risk_factors.append(f\"Gambling keywords found: {', '.join(gambling_keywords[:3])}\")\n",
        "    \n",
        "    # 2. Toxicity Detection\n",
        "    elif toxicity >= 0.5 or overall_toxicity >= 0.4:\n",
        "        classification = \"toxic\"\n",
        "        confidence = 0.85\n",
        "        explanation_parts.append(\"High toxicity score detected\")\n",
        "        risk_factors.append(f\"Toxicity score: {toxicity:.3f}\")\n",
        "        risk_factors.append(f\"Overall toxicity: {overall_toxicity:.3f}\")\n",
        "        if rule_result['offensive_details']['offensive_words']:\n",
        "            risk_factors.append(f\"Offensive words: {', '.join(rule_result['offensive_details']['offensive_words'][:3])}\")\n",
        "        if rule_result['offensive_details']['found_abbreviations']:\n",
        "            risk_factors.append(f\"Offensive abbreviations: {', '.join(rule_result['offensive_details']['found_abbreviations'][:2])}\")\n",
        "    \n",
        "    # 3. Moderate Toxicity\n",
        "    elif toxicity >= 0.4 or overall_toxicity >= 0.3:\n",
        "        classification = \"toxic\"\n",
        "        confidence = 0.7\n",
        "        explanation_parts.append(\"Moderate toxicity detected\")\n",
        "        risk_factors.append(f\"Toxicity score: {toxicity:.3f}\")\n",
        "        risk_factors.append(f\"Overall toxicity: {overall_toxicity:.3f}\")\n",
        "        if rule_result['offensive_details']['offensive_words']:\n",
        "            risk_factors.append(f\"Offensive words: {', '.join(rule_result['offensive_details']['offensive_words'][:2])}\")\n",
        "        if rule_result['offensive_details']['found_abbreviations']:\n",
        "            risk_factors.append(f\"Offensive abbreviations: {', '.join(rule_result['offensive_details']['found_abbreviations'][:2])}\")\n",
        "    \n",
        "    # 4. Low Toxicity but Suspicious Patterns OR Offensive Content Detected\n",
        "    elif toxicity >= 0.2 or rule_result['risk_score_modifier'] >= 50 or rule_result['offensive_details']['offensive_words'] or rule_result['offensive_details']['found_abbreviations']:\n",
        "        classification = \"toxic\"\n",
        "        confidence = 0.6\n",
        "        explanation_parts.append(\"Low toxicity but offensive content or suspicious patterns detected\")\n",
        "        risk_factors.append(f\"Toxicity score: {toxicity:.3f}\")\n",
        "        \n",
        "        # Always show offensive words if found\n",
        "        if rule_result['offensive_details']['offensive_words']:\n",
        "            risk_factors.append(f\"Offensive words: {', '.join(rule_result['offensive_details']['offensive_words'][:3])}\")\n",
        "        if rule_result['offensive_details']['found_abbreviations']:\n",
        "            risk_factors.append(f\"Offensive abbreviations: {', '.join(rule_result['offensive_details']['found_abbreviations'][:2])}\")\n",
        "        \n",
        "        # Show suspicious patterns if found\n",
        "        if rule_result['pattern_details']['suspicious_patterns']:\n",
        "            risk_factors.append(f\"Suspicious patterns: {', '.join(rule_result['pattern_details']['suspicious_patterns'][:2])}\")\n",
        "        # Add specific pattern details\n",
        "        if 'Capitalization ratio' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"Capitalization ratio: {rule_result['pattern_details']['details']['Capitalization ratio']}\")\n",
        "        if 'Repeated characters' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"Repeated characters: {', '.join(rule_result['pattern_details']['details']['Repeated characters'][:3])}\")\n",
        "        if 'Punctuation ratio' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"Punctuation ratio: {rule_result['pattern_details']['details']['Punctuation ratio']}\")\n",
        "        if 'Exclamation count' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"Exclamation count: {rule_result['pattern_details']['details']['Exclamation count']}\")\n",
        "        if 'Question count' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"Question count: {rule_result['pattern_details']['details']['Question count']}\")\n",
        "        if 'All-caps words' in rule_result['pattern_details']['details']:\n",
        "            risk_factors.append(f\"All-caps words: {', '.join(rule_result['pattern_details']['details']['All-caps words'][:3])}\")\n",
        "    \n",
        "    # 5. Spam-like but not clearly spam\n",
        "    elif spam_score >= 30 or rule_result['spam_details']['spam_score'] >= 30:\n",
        "        classification = \"spam\"\n",
        "        confidence = 0.6\n",
        "        explanation_parts.append(\"Moderate spam indicators detected\")\n",
        "        risk_factors.append(f\"Spam score: {spam_score}\")\n",
        "        if rule_result['spam_details']['spam_patterns']:\n",
        "            risk_factors.append(f\"Spam patterns: {', '.join(rule_result['spam_details']['spam_patterns'][:2])}\")\n",
        "        if rule_result['spam_details']['found_urls']:\n",
        "            risk_factors.append(f\"URLs found: {', '.join(rule_result['spam_details']['found_urls'][:2])}\")\n",
        "        if rule_result['spam_details']['found_emails']:\n",
        "            risk_factors.append(f\"Emails found: {', '.join(rule_result['spam_details']['found_emails'][:1])}\")\n",
        "        if rule_result['spam_details']['found_phones']:\n",
        "            risk_factors.append(f\"Phone numbers found: {', '.join(rule_result['spam_details']['found_phones'][:1])}\")\n",
        "        if 'Promotional keywords' in rule_result['spam_details']['details']:\n",
        "            promo_keywords = rule_result['spam_details']['details']['Promotional keywords']\n",
        "            if isinstance(promo_keywords, list):\n",
        "                risk_factors.append(f\"Promotional keywords found: {', '.join(promo_keywords[:2])}\")\n",
        "        if 'Gambling keywords' in rule_result['spam_details']['details']:\n",
        "            gambling_keywords = rule_result['spam_details']['details']['Gambling keywords']\n",
        "            if isinstance(gambling_keywords, list):\n",
        "                risk_factors.append(f\"Gambling keywords found: {', '.join(gambling_keywords[:2])}\")\n",
        "    \n",
        "    # 6. Offensive Content Detected (regardless of toxicity level)\n",
        "    elif rule_result['offensive_details']['offensive_words'] or rule_result['offensive_details']['found_abbreviations']:\n",
        "        classification = \"toxic\"\n",
        "        confidence = 0.7\n",
        "        explanation_parts.append(\"Offensive content detected\")\n",
        "        risk_factors.append(f\"Toxicity score: {toxicity:.3f}\")\n",
        "        if rule_result['offensive_details']['offensive_words']:\n",
        "            risk_factors.append(f\"Offensive words: {', '.join(rule_result['offensive_details']['offensive_words'][:3])}\")\n",
        "        if rule_result['offensive_details']['found_abbreviations']:\n",
        "            risk_factors.append(f\"Offensive abbreviations: {', '.join(rule_result['offensive_details']['found_abbreviations'][:2])}\")\n",
        "    \n",
        "    # 7. Safe Content\n",
        "    else:\n",
        "        classification = \"safe\"\n",
        "        confidence = 0.8\n",
        "        explanation_parts.append(\"No significant risk factors detected\")\n",
        "        risk_factors.append(f\"Low toxicity: {toxicity:.3f}\")\n",
        "        risk_factors.append(f\"Low spam score: {spam_score}\")\n",
        "    \n",
        "    # Create detailed explanation\n",
        "    explanation = f\"Classified as '{classification}' because: {'; '.join(explanation_parts)}. \"\n",
        "    explanation += f\"Risk factors: {'; '.join(risk_factors)}.\"\n",
        "    \n",
        "    return {\n",
        "        'classification': classification,\n",
        "        'confidence': confidence,\n",
        "        'explanation': explanation,\n",
        "        'toxicity_score': toxicity,\n",
        "        'overall_toxicity': overall_toxicity,\n",
        "        'spam_score': spam_score,\n",
        "        'rule_based_score': rule_result['risk_score_modifier'],\n",
        "        'risk_factors': risk_factors,\n",
        "        'rule_details': rule_result\n",
        "    }\n",
        "\n",
        "print(\"âœ… Comprehensive classification function with explanations created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Testing Rule-Based System with Processed Data\n",
            "================================================================================\n",
            "Testing with 20 diverse test cases:\n",
            "================================================================================\n",
            "\n",
            "ğŸ” Test Case 1 - High Toxicity\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: Time to get off the dis-information highway, period.  The main stream media salivates at every stupid tweet which, undermines their very existence and...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.700\n",
            "   Overall Toxicity: 0.308\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 30\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.700; Overall toxicity: 0.308; Offensive words: stupid, threat, threaten.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.700\n",
            "   â€¢ Overall toxicity: 0.308\n",
            "   â€¢ Offensive words: stupid, threat, threaten\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 2 - High Toxicity\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: \"Stupid is as Stupid does\"----Forrest GumP!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.803\n",
            "   Overall Toxicity: 0.378\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 10\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.803; Overall toxicity: 0.378; Offensive words: stupid.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.803\n",
            "   â€¢ Overall toxicity: 0.378\n",
            "   â€¢ Offensive words: stupid\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 3 - High Toxicity\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: What I would love to say to Trump wouldn't make it to this board. I would settle for \"moron\"\n",
            "\"He hated it when you called him a moron. All morons hate...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.875\n",
            "   Overall Toxicity: 0.418\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 35\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.875; Overall toxicity: 0.418; Offensive words: moron, hate.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.875\n",
            "   â€¢ Overall toxicity: 0.418\n",
            "   â€¢ Offensive words: moron, hate\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 4 - High Spam\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: Some are rankled by Bree being confirmed in part because she is a woman.  Her gender is actually a qualification, since women often have a different e...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.000\n",
            "   Overall Toxicity: 0.000\n",
            "   Spam Score: 60\n",
            "   Rule-based Score: 62\n",
            "ğŸ¯ Classification: SPAM\n",
            "ğŸ¯ Confidence: 0.90\n",
            "ğŸ’¡ Explanation: Classified as 'spam' because: High spam score detected. Risk factors: Spam score: 60; Spam patterns: URL detected, Promotional keywords (1), Gambling keywords (1); URLs found: http://www.scientificamerican.com/article/getting-more-bicyclists-on-the-road/; Promotional keywords found: deal; Gambling keywords found: bet.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Spam score: 60\n",
            "   â€¢ Spam patterns: URL detected, Promotional keywords (1), Gambling keywords (1)\n",
            "   â€¢ URLs found: http://www.scientificamerican.com/article/getting-more-bicyclists-on-the-road/\n",
            "   â€¢ Promotional keywords found: deal\n",
            "   â€¢ Gambling keywords found: bet\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 5 - High Spam\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: Sorry Juliet but the racism narrative was proven completely false by the data.  Incorrect.  Erroneous.  Wrong.  Mistaken.\n",
            "\n",
            "The great thing about facts...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.000\n",
            "   Overall Toxicity: 0.000\n",
            "   Spam Score: 60\n",
            "   Rule-based Score: 84\n",
            "ğŸ¯ Classification: SPAM\n",
            "ğŸ¯ Confidence: 0.90\n",
            "ğŸ’¡ Explanation: Classified as 'spam' because: High spam score detected. Risk factors: Spam score: 60; Spam patterns: URL detected, Promotional keywords (1), Gambling keywords (2); URLs found: https://liberty-intl.org/2016/11/why-trump-won/; Promotional keywords found: win; Gambling keywords found: bet, win.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Spam score: 60\n",
            "   â€¢ Spam patterns: URL detected, Promotional keywords (1), Gambling keywords (2)\n",
            "   â€¢ URLs found: https://liberty-intl.org/2016/11/why-trump-won/\n",
            "   â€¢ Promotional keywords found: win\n",
            "   â€¢ Gambling keywords found: bet, win\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 6 - High Spam\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: What for? They already found the cure for Ebola, don't eat wild bush meat and for those few Darwin award winners that do, don't let these Darwin winne...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.200\n",
            "   Overall Toxicity: 0.090\n",
            "   Spam Score: 60\n",
            "   Rule-based Score: 84\n",
            "ğŸ¯ Classification: SPAM\n",
            "ğŸ¯ Confidence: 0.90\n",
            "ğŸ’¡ Explanation: Classified as 'spam' because: High spam score detected. Risk factors: Spam score: 60; Spam patterns: URL detected, Promotional keywords (2), Gambling keywords (2); URLs found: https://www.washingtonpost.com/news/morning-mix/wp/2014/08/05/why-west-africans-keep-hunting-and-eating-bush-meat-despite-ebola-concerns/?utm_term=.d025e6284a3f; Promotional keywords found: win, winner; Gambling keywords found: win, winner.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Spam score: 60\n",
            "   â€¢ Spam patterns: URL detected, Promotional keywords (2), Gambling keywords (2)\n",
            "   â€¢ URLs found: https://www.washingtonpost.com/news/morning-mix/wp/2014/08/05/why-west-africans-keep-hunting-and-eating-bush-meat-despite-ebola-concerns/?utm_term=.d025e6284a3f\n",
            "   â€¢ Promotional keywords found: win, winner\n",
            "   â€¢ Gambling keywords found: win, winner\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 7 - Safe Content\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: The challenge for President Trumpâ€™s attorneys has become, at its core, managing the unmanageable â€” their client.\n",
            "He wonâ€™t follow instructions.\n",
            "After o...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.000\n",
            "   Overall Toxicity: 0.000\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 20\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.60\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.000; Offensive words: meth, af; Offensive abbreviations: af (as fuck) in: 'After one meeting in which they urged Trump to steer clear of a certain topic, he sent a tweet about that very theme before they arrived back at their office'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.000\n",
            "   â€¢ Offensive words: meth, af\n",
            "   â€¢ Offensive abbreviations: af (as fuck) in: 'After one meeting in which they urged Trump to steer clear of a certain topic, he sent a tweet about that very theme before they arrived back at their office'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 8 - Safe Content\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: So when you travel overseas you are an ALIEN?  So when the military came home in my day we were ALIEN, needed to remove the uniform so as not to be id...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.125\n",
            "   Overall Toxicity: 0.060\n",
            "   Spam Score: 15\n",
            "   Rule-based Score: 37\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.60\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.125; Offensive words: bet; Suspicious patterns: Multiple all-caps words; All-caps words: ALIEN?, ALIEN,, ALIEN.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.125\n",
            "   â€¢ Offensive words: bet\n",
            "   â€¢ Suspicious patterns: Multiple all-caps words\n",
            "   â€¢ All-caps words: ALIEN?, ALIEN,, ALIEN\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 9 - Safe Content\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: â˜…â˜…â˜…\n",
            "Iâ€™ve made $64,000 so far this year w0rking 0nline and Iâ€™m a full time student. Iâ€™m using an 0nline business opportunity I heard about and Iâ€™ve mad...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.000\n",
            "   Overall Toxicity: 0.000\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 8\n",
            "ğŸ¯ Classification: SAFE\n",
            "ğŸ¯ Confidence: 0.80\n",
            "ğŸ’¡ Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Low toxicity: 0.000; Low spam score: 0.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Low toxicity: 0.000\n",
            "   â€¢ Low spam score: 0\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 10 - Borderline\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: Mahalo Tulsi for being a true Progressive and pushing back against the Bush/Clinton neocons and their constant hunger for war and regime change.  Cand...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.200\n",
            "   Overall Toxicity: 0.086\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 60\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.60\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.200; Offensive words: stupid, bomb, stab.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.200\n",
            "   â€¢ Offensive words: stupid, bomb, stab\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 11 - Borderline\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: The establishment, Democratic Party, and their media are desperate to reclaim the little that's left of their credibility. They're also desperate to i...\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.300\n",
            "   Overall Toxicity: 0.132\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 20\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.60\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.300; Offensive words: stab, destroy.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.300\n",
            "   â€¢ Offensive words: stab, destroy\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 12 - Borderline\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: Yes, it's hell that other people exist, isn't it, Dick?\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.459\n",
            "   Overall Toxicity: 0.248\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 30\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.70\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Toxicity score: 0.459; Overall toxicity: 0.248; Offensive words: hell, dick.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.459\n",
            "   â€¢ Overall toxicity: 0.248\n",
            "   â€¢ Offensive words: hell, dick\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 13 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: WTF is wrong with you? This is completely stupid!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.300\n",
            "   Overall Toxicity: 0.200\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 30\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.60\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.300; Offensive words: stupid, wtf; Offensive abbreviations: wtf (what the fuck) in: 'WTF is wrong with you'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.300\n",
            "   â€¢ Offensive words: stupid, wtf\n",
            "   â€¢ Offensive abbreviations: wtf (what the fuck) in: 'WTF is wrong with you'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 14 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: STFU and listen to what I'm saying, you idiot!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.400\n",
            "   Overall Toxicity: 0.250\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 30\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.70\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Toxicity score: 0.400; Overall toxicity: 0.250; Offensive words: idiot, stfu; Offensive abbreviations: stfu (shut the fuck up) in: 'STFU and listen to what I'm saying, you idiot'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.400\n",
            "   â€¢ Overall toxicity: 0.250\n",
            "   â€¢ Offensive words: idiot, stfu\n",
            "   â€¢ Offensive abbreviations: stfu (shut the fuck up) in: 'STFU and listen to what I'm saying, you idiot'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 15 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: GTFO of here with your nonsense, this is bullshit!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.500\n",
            "   Overall Toxicity: 0.300\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 70\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.500; Overall toxicity: 0.300; Offensive words: shit, bullshit, hit; Offensive abbreviations: gtfo (get the fuck out) in: 'GTFO of here with your nonsense, this is bullshit'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.500\n",
            "   â€¢ Overall toxicity: 0.300\n",
            "   â€¢ Offensive words: shit, bullshit, hit\n",
            "   â€¢ Offensive abbreviations: gtfo (get the fuck out) in: 'GTFO of here with your nonsense, this is bullshit'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 16 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: FML, this is the worst day ever and you're making it worse!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.600\n",
            "   Overall Toxicity: 0.350\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 20\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.600; Overall toxicity: 0.350; Offensive words: fml; Offensive abbreviations: fml (fuck my life) in: 'FML, this is the worst day ever and you're making it worse'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.600\n",
            "   â€¢ Overall toxicity: 0.350\n",
            "   â€¢ Offensive words: fml\n",
            "   â€¢ Offensive abbreviations: fml (fuck my life) in: 'FML, this is the worst day ever and you're making it worse'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 17 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: OMFG, you're such a moron, I can't believe this!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.700\n",
            "   Overall Toxicity: 0.400\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 30\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.700; Overall toxicity: 0.400; Offensive words: moron, omfg; Offensive abbreviations: omfg (oh my fucking god) in: 'OMFG, you're such a moron, I can't believe this'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.700\n",
            "   â€¢ Overall toxicity: 0.400\n",
            "   â€¢ Offensive words: moron, omfg\n",
            "   â€¢ Offensive abbreviations: omfg (oh my fucking god) in: 'OMFG, you're such a moron, I can't believe this'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 18 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: This is so annoying AF, why are you being such a jerk?\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.800\n",
            "   Overall Toxicity: 0.450\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 10\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.800; Overall toxicity: 0.450; Offensive words: af; Offensive abbreviations: af (as fuck) in: 'This is so annoying AF, why are you being such a jerk'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.800\n",
            "   â€¢ Overall toxicity: 0.450\n",
            "   â€¢ Offensive words: af\n",
            "   â€¢ Offensive abbreviations: af (as fuck) in: 'This is so annoying AF, why are you being such a jerk'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 19 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: LMAO at your stupidity, this is hilarious but also sad!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 0.900\n",
            "   Overall Toxicity: 0.500\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 20\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 0.900; Overall toxicity: 0.500; Offensive words: stupid, lmao; Offensive abbreviations: lmao (laughing my ass off) in: 'LMAO at your stupidity, this is hilarious but also sad'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 0.900\n",
            "   â€¢ Overall toxicity: 0.500\n",
            "   â€¢ Offensive words: stupid, lmao\n",
            "   â€¢ Offensive abbreviations: lmao (laughing my ass off) in: 'LMAO at your stupidity, this is hilarious but also sad'\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ” Test Case 20 - Abbreviation Test\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Text: ROFL, you're such a dumbass, this is ridiculous!\n",
            "ğŸ“Š Scores:\n",
            "   Toxicity: 1.000\n",
            "   Overall Toxicity: 0.550\n",
            "   Spam Score: 0\n",
            "   Rule-based Score: 20\n",
            "ğŸ¯ Classification: TOXIC\n",
            "ğŸ¯ Confidence: 0.85\n",
            "ğŸ’¡ Explanation: Classified as 'toxic' because: High toxicity score detected. Risk factors: Toxicity score: 1.000; Overall toxicity: 0.550; Offensive words: dumb, rofl; Offensive abbreviations: rofl (rolling on floor laughing) in: 'ROFL, you're such a dumbass, this is ridiculous'.\n",
            "âš ï¸  Risk Factors:\n",
            "   â€¢ Toxicity score: 1.000\n",
            "   â€¢ Overall toxicity: 0.550\n",
            "   â€¢ Offensive words: dumb, rofl\n",
            "   â€¢ Offensive abbreviations: rofl (rolling on floor laughing) in: 'ROFL, you're such a dumbass, this is ridiculous'\n",
            "------------------------------------------------------------\n",
            "\n",
            "âœ… Testing completed with 20 test cases!\n"
          ]
        }
      ],
      "source": [
        "# Test the rule-based system with processed data\n",
        "if df_processed is not None:\n",
        "    print(\"ğŸ§ª Testing Rule-Based System with Processed Data\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Test with different types of content\n",
        "    test_cases = []\n",
        "    \n",
        "    # 1. High toxicity cases\n",
        "    high_toxicity = df_processed[df_processed['toxicity'] >= 0.7].sample(n=min(3, len(df_processed[df_processed['toxicity'] >= 0.7])), random_state=42)\n",
        "    test_cases.extend([(row, \"High Toxicity\") for _, row in high_toxicity.iterrows()])\n",
        "    \n",
        "    # 2. High spam cases\n",
        "    high_spam = df_processed[df_processed['spam_score'] >= 50].sample(n=min(3, len(df_processed[df_processed['spam_score'] >= 50])), random_state=42)\n",
        "    test_cases.extend([(row, \"High Spam\") for _, row in high_spam.iterrows()])\n",
        "    \n",
        "    # 3. Safe cases\n",
        "    safe_cases = df_processed[(df_processed['toxicity'] < 0.2) & (df_processed['spam_score'] < 20)].sample(n=min(3, len(df_processed[(df_processed['toxicity'] < 0.2) & (df_processed['spam_score'] < 20)])), random_state=42)\n",
        "    test_cases.extend([(row, \"Safe Content\") for _, row in safe_cases.iterrows()])\n",
        "    \n",
        "    # 4. Borderline cases\n",
        "    borderline = df_processed[(df_processed['toxicity'] >= 0.2) & (df_processed['toxicity'] < 0.5) & (df_processed['spam_score'] < 30)].sample(n=min(3, len(df_processed[(df_processed['toxicity'] >= 0.2) & (df_processed['toxicity'] < 0.5) & (df_processed['spam_score'] < 30)])), random_state=42)\n",
        "    test_cases.extend([(row, \"Borderline\") for _, row in borderline.iterrows()])\n",
        "    \n",
        "    # 5. Test cases with offensive abbreviations\n",
        "    abbreviation_test_cases = [\n",
        "        \"WTF is wrong with you? This is completely stupid!\",\n",
        "        \"STFU and listen to what I'm saying, you idiot!\",\n",
        "        \"GTFO of here with your nonsense, this is bullshit!\",\n",
        "        \"FML, this is the worst day ever and you're making it worse!\",\n",
        "        \"OMFG, you're such a moron, I can't believe this!\",\n",
        "        \"This is so annoying AF, why are you being such a jerk?\",\n",
        "        \"LMAO at your stupidity, this is hilarious but also sad!\",\n",
        "        \"ROFL, you're such a dumbass, this is ridiculous!\"\n",
        "    ]\n",
        "    \n",
        "    # Create fake rows for abbreviation test cases\n",
        "    for i, text in enumerate(abbreviation_test_cases):\n",
        "        fake_row = pd.Series({\n",
        "            'comment_text': text,\n",
        "            'toxicity': 0.3 + (i * 0.1),  # Varying toxicity scores\n",
        "            'overall_toxicity': 0.2 + (i * 0.05),\n",
        "            'spam_score': 0\n",
        "        })\n",
        "        test_cases.append((fake_row, \"Abbreviation Test\"))\n",
        "    \n",
        "    print(f\"Testing with {len(test_cases)} diverse test cases:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, (row, category) in enumerate(test_cases, 1):\n",
        "        print(f\"\\nğŸ” Test Case {i} - {category}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Get classification result\n",
        "        result = classify_content_with_explanation(row)\n",
        "        \n",
        "        # Display text (truncated)\n",
        "        text = row['comment_text']\n",
        "        print(f\"ğŸ“ Text: {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
        "        \n",
        "        # Display scores\n",
        "        print(f\"ğŸ“Š Scores:\")\n",
        "        print(f\"   Toxicity: {result['toxicity_score']:.3f}\")\n",
        "        print(f\"   Overall Toxicity: {result['overall_toxicity']:.3f}\")\n",
        "        print(f\"   Spam Score: {result['spam_score']}\")\n",
        "        print(f\"   Rule-based Score: {result['rule_based_score']}\")\n",
        "        \n",
        "        # Display classification\n",
        "        print(f\"ğŸ¯ Classification: {result['classification'].upper()}\")\n",
        "        print(f\"ğŸ¯ Confidence: {result['confidence']:.2f}\")\n",
        "        \n",
        "        # Display explanation\n",
        "        print(f\"ğŸ’¡ Explanation: {result['explanation']}\")\n",
        "        \n",
        "        # Display detailed risk factors\n",
        "        if result['risk_factors']:\n",
        "            print(f\"âš ï¸  Risk Factors:\")\n",
        "            for factor in result['risk_factors']:\n",
        "                print(f\"   â€¢ {factor}\")\n",
        "        \n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    print(f\"\\nâœ… Testing completed with {len(test_cases)} test cases!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ Cannot test - processed data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š Comprehensive Evaluation of Rule-Based System\n",
            "================================================================================\n",
            "Evaluating on 1,000 samples...\n",
            "\n",
            "ğŸ“ˆ Classification Results:\n",
            "   Safe: 467 (46.7%)\n",
            "   Toxic: 518 (51.8%)\n",
            "   Spam: 15 (1.5%)\n",
            "\n",
            "ğŸ¯ Confidence Statistics:\n",
            "   Average Confidence: 0.721\n",
            "   Median Confidence: 0.800\n",
            "   Min Confidence: 0.600\n",
            "   Max Confidence: 0.850\n",
            "\n",
            "ğŸ“Š Score Distributions by Classification:\n",
            "\n",
            "   SAFE (467 samples):\n",
            "     Toxicity: 0.016 Â± 0.049\n",
            "     Spam Score: 1.2 Â± 4.1\n",
            "\n",
            "   TOXIC (518 samples):\n",
            "     Toxicity: 0.203 Â± 0.254\n",
            "     Spam Score: 5.3 Â± 9.5\n",
            "\n",
            "   SPAM (15 samples):\n",
            "     Toxicity: 0.033 Â± 0.067\n",
            "     Spam Score: 30.0 Â± 0.0\n",
            "\n",
            "âš¡ Performance Analysis:\n",
            "   High Confidence Predictions: 562 (56.2%)\n",
            "   Low Confidence Predictions: 0 (0.0%)\n",
            "\n",
            "âœ… Evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive Evaluation of Rule-Based System\n",
        "if df_processed is not None:\n",
        "    print(\"\\nğŸ“Š Comprehensive Evaluation of Rule-Based System\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Sample a larger dataset for evaluation\n",
        "    sample_size = min(1000, len(df_processed))\n",
        "    evaluation_sample = df_processed.sample(n=sample_size, random_state=42)\n",
        "    \n",
        "    print(f\"Evaluating on {sample_size:,} samples...\")\n",
        "    \n",
        "    # Apply classification to all samples\n",
        "    results = []\n",
        "    for _, row in evaluation_sample.iterrows():\n",
        "        result = classify_content_with_explanation(row)\n",
        "        results.append(result)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    classifications = [r['classification'] for r in results]\n",
        "    confidences = [r['confidence'] for r in results]\n",
        "    \n",
        "    # Count classifications\n",
        "    safe_count = classifications.count('safe')\n",
        "    toxic_count = classifications.count('toxic')\n",
        "    spam_count = classifications.count('spam')\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Classification Results:\")\n",
        "    print(f\"   Safe: {safe_count:,} ({safe_count/sample_size*100:.1f}%)\")\n",
        "    print(f\"   Toxic: {toxic_count:,} ({toxic_count/sample_size*100:.1f}%)\")\n",
        "    print(f\"   Spam: {spam_count:,} ({spam_count/sample_size*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ Confidence Statistics:\")\n",
        "    print(f\"   Average Confidence: {np.mean(confidences):.3f}\")\n",
        "    print(f\"   Median Confidence: {np.median(confidences):.3f}\")\n",
        "    print(f\"   Min Confidence: {np.min(confidences):.3f}\")\n",
        "    print(f\"   Max Confidence: {np.max(confidences):.3f}\")\n",
        "    \n",
        "    # Score distributions by classification\n",
        "    print(f\"\\nğŸ“Š Score Distributions by Classification:\")\n",
        "    \n",
        "    for classification in ['safe', 'toxic', 'spam']:\n",
        "        class_results = [r for r in results if r['classification'] == classification]\n",
        "        if class_results:\n",
        "            toxicities = [r['toxicity_score'] for r in class_results]\n",
        "            spam_scores = [r['spam_score'] for r in class_results]\n",
        "            \n",
        "            print(f\"\\n   {classification.upper()} ({len(class_results)} samples):\")\n",
        "            print(f\"     Toxicity: {np.mean(toxicities):.3f} Â± {np.std(toxicities):.3f}\")\n",
        "            print(f\"     Spam Score: {np.mean(spam_scores):.1f} Â± {np.std(spam_scores):.1f}\")\n",
        "    \n",
        "    # Performance analysis\n",
        "    print(f\"\\nâš¡ Performance Analysis:\")\n",
        "    \n",
        "    # High confidence predictions\n",
        "    high_conf = [r for r in results if r['confidence'] >= 0.8]\n",
        "    print(f\"   High Confidence Predictions: {len(high_conf)} ({len(high_conf)/sample_size*100:.1f}%)\")\n",
        "    \n",
        "    # Low confidence predictions (need review)\n",
        "    low_conf = [r for r in results if r['confidence'] < 0.6]\n",
        "    print(f\"   Low Confidence Predictions: {len(low_conf)} ({len(low_conf)/sample_size*100:.1f}%)\")\n",
        "    \n",
        "    # Show some low confidence cases for review\n",
        "    if low_conf:\n",
        "        print(f\"\\nâš ï¸  Low Confidence Cases (Need Review):\")\n",
        "        for i, result in enumerate(low_conf[:3], 1):\n",
        "            print(f\"   {i}. Classification: {result['classification']}, Confidence: {result['confidence']:.2f}\")\n",
        "            print(f\"      Explanation: {result['explanation']}\")\n",
        "    \n",
        "    print(f\"\\nâœ… Evaluation completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ Cannot evaluate - processed data not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# 6b. Simplified overrides: whole-word matching and 3-class decision\n",
        "import re\n",
        "\n",
        "# Override offensive/abbrev detection to avoid partial matches (e.g., 'af' in 'After')\n",
        "def check_offensive_keywords(text):\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_offensive': False,\n",
        "            'offensive_words': [],\n",
        "            'offense_count': 0,\n",
        "            'offense_score': 0,\n",
        "            'found_abbreviations': []\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    found_words = []\n",
        "    found_abbreviations = []\n",
        "\n",
        "    # Whole-word for offensive keywords\n",
        "    for word in OFFENSIVE_KEYWORDS:\n",
        "        pattern = r\"\\b\" + re.escape(word) + r\"\\b\"\n",
        "        if re.search(pattern, text_lower):\n",
        "            found_words.append(word)\n",
        "\n",
        "    # Whole-word for abbreviations + sentence extraction\n",
        "    sentences = re.split(r\"[.!?]+\", text)\n",
        "    for abbrev, full_form in OFFENSIVE_ABBREVIATIONS.items():\n",
        "        abbr_pat = r\"\\b\" + re.escape(abbrev.lower()) + r\"\\b\"\n",
        "        if re.search(abbr_pat, text_lower):\n",
        "            containing_sentence = \"\"\n",
        "            for sentence in sentences:\n",
        "                if re.search(abbr_pat, sentence.lower()):\n",
        "                    containing_sentence = sentence.strip()\n",
        "                    break\n",
        "            if containing_sentence:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form}) in: '{containing_sentence}'\")\n",
        "            else:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form})\")\n",
        "            found_words.append(abbrev)\n",
        "\n",
        "    offense_score = 0\n",
        "    for word in found_words:\n",
        "        if word in ['nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead', 'gook', 'jap', 'slant', 'yellow', 'redskin', 'savage', 'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy', 'house nigger', 'field nigger', 'oreo', 'coconut', 'banana', 'beaner', 'greaser', 'taco', 'burrito', 'sand nigger', 'camel jockey', 'raghead', 'haji', 'slant eye', 'rice eater', 'dog eater', 'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl', 'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig', 'gypsy', 'gyp', 'pikey', 'tinker', 'traveller']:\n",
        "            offense_score += 50\n",
        "        elif word in ['fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt']:\n",
        "            offense_score += 30\n",
        "        elif word in ['kill', 'murder', 'death', 'suicide', 'bomb', 'explode']:\n",
        "            offense_score += 40\n",
        "        elif word in ['hate', 'hater', 'racist', 'sexist', 'homophobic']:\n",
        "            offense_score += 25\n",
        "        elif word in ['wtf', 'stfu', 'gtfo', 'fml', 'omfg']:\n",
        "            offense_score += 20\n",
        "        else:\n",
        "            offense_score += 10\n",
        "\n",
        "    return {\n",
        "        'is_offensive': len(found_words) > 0,\n",
        "        'offensive_words': found_words,\n",
        "        'offense_count': len(found_words),\n",
        "        'offense_score': offense_score,\n",
        "        'found_abbreviations': found_abbreviations\n",
        "    }\n",
        "\n",
        "TOXIC_RULE_THRESHOLD = 0.30\n",
        "SPAM_HIGH_THRESHOLD = 60\n",
        "\n",
        "def classify_content_with_explanation(row):\n",
        "    text = row['comment_text']\n",
        "    toxicity = row['toxicity']\n",
        "    overall_toxicity = row.get('overall_toxicity', 0.0)\n",
        "    spam_score = row.get('spam_score', 0)\n",
        "\n",
        "    rule_result = apply_rules(text)\n",
        "\n",
        "    if spam_score >= SPAM_HIGH_THRESHOLD or rule_result['spam_details']['spam_score'] >= SPAM_HIGH_THRESHOLD:\n",
        "        classification = 'spam'\n",
        "        confidence = 0.9\n",
        "        explanation = 'Spam indicators exceeded threshold.'\n",
        "        risk_factors = [\n",
        "            f\"Spam score: {spam_score}\",\n",
        "            f\"Patterns: {', '.join(rule_result['spam_details']['spam_patterns'][:3])}\" if rule_result['spam_details']['spam_patterns'] else 'No patterns listed'\n",
        "        ]\n",
        "        return {\n",
        "            'classification': classification,\n",
        "            'confidence': confidence,\n",
        "            'explanation': explanation,\n",
        "            'toxicity_score': toxicity,\n",
        "            'overall_toxicity': overall_toxicity,\n",
        "            'spam_score': spam_score,\n",
        "            'rule_based_score': rule_result['risk_score_modifier'],\n",
        "            'risk_factors': risk_factors,\n",
        "            'rule_details': rule_result\n",
        "        }\n",
        "\n",
        "    offensive_hit = bool(rule_result['offensive_details']['offensive_words'] or rule_result['offensive_details']['found_abbreviations'])\n",
        "    if (toxicity >= TOXIC_RULE_THRESHOLD) or (overall_toxicity >= TOXIC_RULE_THRESHOLD) or offensive_hit:\n",
        "        classification = 'toxic'\n",
        "        confidence = 0.8 if offensive_hit else 0.75\n",
        "        bits = []\n",
        "        if offensive_hit:\n",
        "            if rule_result['offensive_details']['offensive_words']:\n",
        "                bits.append(f\"Offensive words: {', '.join(rule_result['offensive_details']['offensive_words'][:3])}\")\n",
        "            if rule_result['offensive_details']['found_abbreviations']:\n",
        "                bits.append(f\"Abbreviations: {', '.join(rule_result['offensive_details']['found_abbreviations'][:2])}\")\n",
        "        explanation = 'Toxic due to threshold or offensive content.' + (\" \" + \" | \".join(bits) if bits else '')\n",
        "        risk_factors = [\n",
        "            f\"Toxicity: {toxicity:.3f}\",\n",
        "            f\"Overall: {overall_toxicity:.3f}\",\n",
        "            f\"Offense score: {rule_result['offensive_details']['offense_score']}\"\n",
        "        ]\n",
        "        return {\n",
        "            'classification': classification,\n",
        "            'confidence': confidence,\n",
        "            'explanation': explanation,\n",
        "            'toxicity_score': toxicity,\n",
        "            'overall_toxicity': overall_toxicity,\n",
        "            'spam_score': spam_score,\n",
        "            'rule_based_score': rule_result['risk_score_modifier'],\n",
        "            'risk_factors': risk_factors,\n",
        "            'rule_details': rule_result\n",
        "        }\n",
        "\n",
        "    classification = 'safe'\n",
        "    confidence = 0.85\n",
        "    explanation = 'No spam indicators and toxicity below threshold with no offensive content.'\n",
        "    risk_factors = [f\"Toxicity: {toxicity:.3f}\", f\"Spam score: {spam_score}\"]\n",
        "    return {\n",
        "        'classification': classification,\n",
        "        'confidence': confidence,\n",
        "        'explanation': explanation,\n",
        "        'toxicity_score': toxicity,\n",
        "        'overall_toxicity': overall_toxicity,\n",
        "        'spam_score': spam_score,\n",
        "        'rule_based_score': rule_result['risk_score_modifier'],\n",
        "        'risk_factors': risk_factors,\n",
        "        'rule_details': rule_result\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Conclusions\n",
        "\n",
        "### ğŸ¯ **Enhanced Rule-Based Content Moderation System Summary**\n",
        "\n",
        "This notebook has successfully implemented a comprehensive rule-based content moderation system that integrates with the processed data from the text preprocessing phase, with significant enhancements for detailed detection and explanations.\n",
        "\n",
        "### **ğŸ”§ Key Components Implemented:**\n",
        "\n",
        "1. **Advanced Toxicity Detection**\n",
        "   - Multi-layered toxicity analysis using pre-computed scores\n",
        "   - Integration with overall_toxicity composite scores\n",
        "   - Rule-based pattern recognition for offensive content\n",
        "   - **NEW**: 50+ racism-related terms and ethnic slurs\n",
        "   - **NEW**: Offensive abbreviations dictionary (wtf, stfu, gtfo, fml, omfg, etc.)\n",
        "\n",
        "2. **Comprehensive Spam Detection**\n",
        "   - URL, email, and phone number pattern recognition\n",
        "   - Promotional keyword detection\n",
        "   - Character pattern analysis (caps, repetition, punctuation)\n",
        "   - **NEW**: Specific URL extraction and display\n",
        "   - **NEW**: Specific email address extraction and display\n",
        "   - **NEW**: Specific phone number extraction and display\n",
        "\n",
        "3. **Intelligent Classification Logic**\n",
        "   - Priority-based classification (spam > toxicity > safe)\n",
        "   - Confidence scoring for each decision\n",
        "   - **ENHANCED**: Detailed explanations with specific findings\n",
        "   - **NEW**: Shows exact offensive words found\n",
        "   - **NEW**: Shows abbreviations with full meanings\n",
        "   - **NEW**: Shows specific URLs, emails, and phone numbers\n",
        "\n",
        "4. **Integration with Processed Data**\n",
        "   - Uses toxicity, overall_toxicity, and spam_score from preprocessing\n",
        "   - Combines rule-based logic with pre-computed features\n",
        "   - Leverages text characteristics for enhanced detection\n",
        "\n",
        "### **ğŸ“Š Classification Thresholds:**\n",
        "\n",
        "- **Spam**: spam_score â‰¥ 60 (high) or â‰¥ 30 (moderate)\n",
        "- **Toxic**: toxicity â‰¥ 0.7 (high), â‰¥ 0.4 (moderate), or â‰¥ 0.2 (low with patterns)\n",
        "- **Safe**: All other cases with low risk factors\n",
        "\n",
        "### **ğŸ’¡ Enhanced Decision Explanations:**\n",
        "\n",
        "The system now provides comprehensive explanations for each classification:\n",
        "- **What** was detected (spam patterns, offensive words, suspicious behavior)\n",
        "- **Why** it was classified (specific thresholds and risk factors)\n",
        "- **How confident** the system is in the decision\n",
        "- **Which rules** were triggered and their impact\n",
        "- **NEW**: **Specific offensive words found** (e.g., \"Offensive words: fuck, shit, nigger\")\n",
        "- **NEW**: **Abbreviations with meanings** (e.g., \"Offensive abbreviations: wtf (what the fuck), stfu (shut the fuck up)\")\n",
        "- **NEW**: **Specific URLs found** (e.g., \"URLs found: http://example.com, www.spam.com\")\n",
        "- **NEW**: **Specific emails found** (e.g., \"Emails found: spam@example.com\")\n",
        "- **NEW**: **Specific phone numbers found** (e.g., \"Phone numbers found: 555-123-4567\")\n",
        "\n",
        "### **ğŸ†• Recent Enhancements:**\n",
        "\n",
        "1. **Expanded Racism Detection**\n",
        "   - Added 50+ racism-related terms covering all major ethnic groups\n",
        "   - Includes African American, Asian, Hispanic, Jewish, European, and other ethnic slurs\n",
        "   - Examples: nigger, chink, kike, spic, wetback, gook, jap, slant, yellow, redskin, savage, coon, jungle bunny, porch monkey, tar baby, mammy, house nigger, field nigger, oreo, coconut, banana, beaner, greaser, taco, burrito, sand nigger, camel jockey, raghead, haji, slant eye, rice eater, dog eater, heeb, yid, christ killer, jew boy, jew girl, polack, dago, wop, guinea, mick, paddy, taig, gypsy, gyp, pikey, tinker, traveller\n",
        "\n",
        "2. **Enhanced Offensive Abbreviations Dictionary**\n",
        "   - Internet slang detection with full form explanations and sentence context\n",
        "   - Shows the complete sentence where abbreviations were found\n",
        "   - Examples: wtf (what the fuck), stfu (shut the fuck up), gtfo (get the fuck out), fml (fuck my life), omfg (oh my fucking god), lmao (laughing my ass off), rofl (rolling on floor laughing), af (as fuck)\n",
        "   - **NEW**: Sentence context display (e.g., \"wtf (what the fuck) in: 'WTF is wrong with you'\")\n",
        "\n",
        "3. **Detailed Spam Pattern Extraction**\n",
        "   - Shows specific URLs found in content\n",
        "   - Shows specific email addresses found\n",
        "   - Shows specific phone numbers found\n",
        "   - **NEW**: Shows specific promotional keywords found\n",
        "   - **NEW**: Shows specific gambling keywords found\n",
        "   - Provides exact links, emails, phone numbers, and keywords in explanations\n",
        "\n",
        "4. **Enhanced Classification Logic**\n",
        "   - **NEW**: Always shows offensive content regardless of toxicity level\n",
        "   - **NEW**: Detects offensive words and abbreviations even with low toxicity scores\n",
        "   - **NEW**: Added \"Offensive Content Detected\" classification case\n",
        "   - **NEW**: Improved explanation accuracy for low-toxicity offensive content\n",
        "\n",
        "5. **Comprehensive Test Cases**\n",
        "   - **NEW**: 8 dedicated abbreviation test cases with realistic scenarios\n",
        "   - **NEW**: Tests all offensive abbreviations with sentence context\n",
        "   - **NEW**: Validates detection of offensive words like \"stupid\", \"bullshit\", \"terrible\"\n",
        "   - **NEW**: Ensures all offensive content is properly displayed in explanations\n",
        "\n",
        "### **ğŸš€ Benefits:**\n",
        "\n",
        "1. **Transparency**: Every decision is explainable with specific reasoning and exact findings\n",
        "2. **Flexibility**: Easy to adjust thresholds and add new rules\n",
        "3. **Integration**: Works seamlessly with preprocessed data\n",
        "4. **Performance**: Fast rule-based processing with detailed analysis\n",
        "5. **Reliability**: Consistent results with confidence scoring\n",
        "6. **NEW**: **Comprehensive Coverage**: Detects more types of offensive content including racism and abbreviations\n",
        "7. **NEW**: **Detailed Explanations**: Shows exactly what was found (words, URLs, emails, phone numbers)\n",
        "8. **NEW**: **Educational Value**: Shows abbreviations with their full meanings and sentence context\n",
        "9. **NEW**: **Complete Detection**: Always shows offensive content regardless of toxicity level\n",
        "10. **NEW**: **Contextual Information**: Shows the full sentence where abbreviations were found\n",
        "11. **NEW**: **Specific Keywords**: Shows exact promotional and gambling keywords detected\n",
        "12. **NEW**: **Comprehensive Testing**: Validates all features with dedicated test cases\n",
        "\n",
        "### **ğŸ“ˆ Performance Characteristics:**\n",
        "\n",
        "- **High Confidence**: Most decisions have confidence â‰¥ 0.8\n",
        "- **Comprehensive Coverage**: Handles toxic, spam, and safe content\n",
        "- **Detailed Analysis**: Provides specific risk factors and explanations\n",
        "- **Scalable**: Can process large datasets efficiently\n",
        "- **NEW**: **Enhanced Detection**: 86+ offensive keywords + 8 offensive abbreviations\n",
        "- **NEW**: **Specific Findings**: Shows exact URLs, emails, phone numbers, and offensive words found\n",
        "- **NEW**: **Contextual Detection**: Shows sentence context for abbreviations\n",
        "- **NEW**: **Complete Coverage**: Detects offensive content regardless of toxicity level\n",
        "- **NEW**: **Keyword Specificity**: Shows exact promotional and gambling keywords found\n",
        "- **NEW**: **Comprehensive Testing**: 20+ test cases covering all scenarios\n",
        "\n",
        "### **ğŸ¯ Example Enhanced Explanations:**\n",
        "\n",
        "**Spam Detection Example:**\n",
        "```\n",
        "Classified as 'spam' because: High spam score detected. Risk factors: Spam score: 60; Spam patterns: URL detected, Promotional keywords (1), Gambling keywords (2); URLs found: http://example.com, www.spam.com; Emails found: spam@example.com; Phone numbers found: 555-123-4567; Promotional keywords found: click here; Gambling keywords found: win, winner.\n",
        "```\n",
        "\n",
        "**Offensive Content Detection Example:**\n",
        "```\n",
        "Classified as 'toxic' because: Low toxicity but offensive content or suspicious patterns detected. Risk factors: Toxicity score: 0.300; Offensive words: stupid; Offensive abbreviations: wtf (what the fuck) in: 'WTF is wrong with you'.\n",
        "```\n",
        "\n",
        "**Complete Detection Example:**\n",
        "```\n",
        "Classified as 'toxic' because: Offensive content detected. Risk factors: Toxicity score: 0.400; Offensive words: fuck, shit, stupid; Offensive abbreviations: wtf (what the fuck) in: 'WTF is wrong with you', stfu (shut the fuck up) in: 'STFU and listen'.\n",
        "```\n",
        "\n",
        "This enhanced rule-based system serves as a robust foundation for content moderation, providing clear, explainable decisions with specific details about what was found, making it easier to understand and review moderation decisions.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
