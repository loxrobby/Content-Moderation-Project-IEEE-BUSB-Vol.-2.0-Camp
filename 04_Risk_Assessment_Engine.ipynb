{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Risk Assessment Engine\n",
        "\n",
        "This notebook implements a comprehensive risk assessment engine that combines the trained Naive Bayes model with the rule-based filter for robust content moderation.\n",
        "\n",
        "## 🎯 Features:\n",
        "1. **Hybrid Classification**: Combines rule-based filter (higher weight) with Naive Bayes model (lower weight) for toxic/safe classification\n",
        "2. **Spam Detection**: Uses rule-based filter only for spam detection\n",
        "3. **Weighted Decision Making**: Rule-based filter gets 70% weight, Naive Bayes model gets 30% weight for toxic/safe decisions\n",
        "4. **Detailed Explanations**: Explains the reasoning behind each decision with specific evidence\n",
        "5. **Comprehensive Risk Assessment**: Provides confidence levels and risk factors\n",
        "\n",
        "## 📊 Integration:\n",
        "- **Naive Bayes Model**: Uses `naive_bayes_model.pkl`, `naive_bayes_vectorizer.pkl`, and `naive_bayes_label_encoder.pkl`\n",
        "- **Rule-Based Filter**: Uses functions from `02_Rule_Based_Filter.ipynb`\n",
        "- **Weighted Classification**: 70% rule-based + 30% ML model for toxic/safe\n",
        "- **Spam Detection**: 100% rule-based filter for spam classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📚 Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"📚 Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Naive Bayes Model and Rule-Based Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Loading Naive Bayes model and components...\n",
            "✅ Naive Bayes model and components loaded successfully!\n",
            "   - Model type: <class 'sklearn.naive_bayes.MultinomialNB'>\n",
            "   - Model classes: [0 1]\n",
            "   - Vectorizer vocabulary size: 10000\n",
            "   - Label encoder classes: ['safe' 'toxic']\n",
            "   - Numerical features: 7\n",
            "   - Toxicity threshold: 0.25\n",
            "\n",
            "Loading rule-based functions...\n",
            "✓ Rule-based functions loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load Naive Bayes model and components\n",
        "print(\"🔄 Loading Naive Bayes model and components...\")\n",
        "try:\n",
        "    naive_bayes_model = joblib.load('naive_bayes_model.pkl')\n",
        "    vectorizer = joblib.load('naive_bayes_vectorizer.pkl')\n",
        "    label_encoder = joblib.load('naive_bayes_label_encoder.pkl')\n",
        "    numerical_features = joblib.load('naive_bayes_numerical_features.pkl')\n",
        "    toxicity_threshold = joblib.load('naive_bayes_toxicity_threshold.pkl')\n",
        "    print(\"✅ Naive Bayes model and components loaded successfully!\")\n",
        "    print(f\"   - Model type: {type(naive_bayes_model)}\")\n",
        "    print(f\"   - Model classes: {naive_bayes_model.classes_}\")\n",
        "    print(f\"   - Vectorizer vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "    print(f\"   - Label encoder classes: {label_encoder.classes_}\")\n",
        "    print(f\"   - Numerical features: {len(numerical_features)}\")\n",
        "    print(f\"   - Toxicity threshold: {toxicity_threshold}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ Error loading models: {e}\")\n",
        "    print(\"Please ensure these files exist:\")\n",
        "    print(\"   - naive_bayes_model.pkl\")\n",
        "    print(\"   - naive_bayes_vectorizer.pkl\") \n",
        "    print(\"   - naive_bayes_label_encoder.pkl\")\n",
        "    print(\"   - naive_bayes_numerical_features.pkl\")\n",
        "    print(\"   - naive_bayes_toxicity_threshold.pkl\")\n",
        "    print(\"\\n💡 Run the model saving cell in 03_Machine_Learning_Classifier.ipynb first!\")\n",
        "    raise\n",
        "\n",
        "# Import rule-based functions from the second notebook\n",
        "# (In a real deployment, these would be imported from a module)\n",
        "print(\"\\nLoading rule-based functions...\")\n",
        "\n",
        "# Copy the rule-based functions here for standalone use\n",
        "def check_offensive_keywords(text):\n",
        "    \"\"\"Check if text contains offensive keywords.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {'is_offensive': False, 'offensive_words': [], 'offense_count': 0, 'offense_score': 0}\n",
        "    \n",
        "    # Simplified offensive keywords list\n",
        "    offensive_keywords = [\n",
        "        'fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt', 'piss',\n",
        "        'crap', 'hell', 'dick', 'pussy', 'cock', 'whore', 'slut', 'fag',\n",
        "        'nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead',\n",
        "        'retard', 'retarded', 'moron', 'idiot', 'stupid', 'dumb', 'fool',\n",
        "        'kill', 'murder', 'death', 'die', 'suicide', 'bomb', 'explode',\n",
        "        'hate', 'hater', 'racist', 'sexist', 'homophobic', 'transphobic'\n",
        "    ]\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    found_words = []\n",
        "    \n",
        "    for word in offensive_keywords:\n",
        "        if word in text_lower:\n",
        "            found_words.append(word)\n",
        "    \n",
        "    offense_score = len(found_words) * 10  # Simple scoring\n",
        "    \n",
        "    return {\n",
        "        'is_offensive': len(found_words) > 0,\n",
        "        'offensive_words': found_words,\n",
        "        'offense_count': len(found_words),\n",
        "        'offense_score': offense_score\n",
        "    }\n",
        "\n",
        "def detect_spam_patterns(text):\n",
        "    \"\"\"Detect spam patterns in text.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {'is_spam_pattern': False, 'spam_patterns': [], 'spam_score': 0}\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    spam_patterns = []\n",
        "    spam_score = 0\n",
        "    \n",
        "    # URL patterns\n",
        "    if re.search(r'http[s]?://', text_lower) or re.search(r'www\\\\.', text_lower):\n",
        "        spam_patterns.append('URL detected')\n",
        "        spam_score += 30\n",
        "    \n",
        "    # Email patterns\n",
        "    if re.search(r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b', text):\n",
        "        spam_patterns.append('Email address')\n",
        "        spam_score += 20\n",
        "    \n",
        "    # Phone patterns\n",
        "    if re.search(r'\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b', text):\n",
        "        spam_patterns.append('Phone number')\n",
        "        spam_score += 25\n",
        "    \n",
        "    # Spam keywords\n",
        "    spam_keywords = [\n",
        "        'buy now', 'click here', 'free money', 'make money', 'earn money',\n",
        "        'work from home', 'get rich', 'quick cash', 'easy money',\n",
        "        'guaranteed', 'no risk', 'limited time', 'act now',\n",
        "        'special offer', 'discount', 'sale', 'promotion', 'deal',\n",
        "        'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
        "        'casino', 'gambling', 'bet', 'poker', 'slots', 'lottery'\n",
        "    ]\n",
        "    \n",
        "    keyword_count = sum(1 for keyword in spam_keywords if keyword in text_lower)\n",
        "    if keyword_count > 0:\n",
        "        spam_patterns.append(f'Spam keywords ({keyword_count})')\n",
        "        spam_score += keyword_count * 10\n",
        "    \n",
        "    return {\n",
        "        'is_spam_pattern': len(spam_patterns) > 0,\n",
        "        'spam_patterns': spam_patterns,\n",
        "        'spam_score': spam_score\n",
        "    }\n",
        "\n",
        "def check_character_patterns(text):\n",
        "    \"\"\"Check for suspicious character patterns.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {'is_suspicious_pattern': False, 'suspicious_patterns': [], 'pattern_score': 0}\n",
        "    \n",
        "    suspicious_patterns = []\n",
        "    pattern_score = 0\n",
        "    \n",
        "    # Excessive capitalization\n",
        "    if len(text) > 0:\n",
        "        caps_ratio = sum(1 for c in text if c.isupper()) / len(text)\n",
        "        if caps_ratio > 0.7:\n",
        "            suspicious_patterns.append('Excessive capitalization')\n",
        "            pattern_score += 25\n",
        "    \n",
        "    # Repeated characters\n",
        "    if re.search(r'(.)\\\\1{2,}', text):\n",
        "        suspicious_patterns.append('Repeated characters')\n",
        "        pattern_score += 20\n",
        "    \n",
        "    # Excessive punctuation\n",
        "    if len(text) > 0:\n",
        "        punct_ratio = sum(1 for c in text if c in string.punctuation) / len(text)\n",
        "        if punct_ratio > 0.3:\n",
        "            suspicious_patterns.append('Excessive punctuation')\n",
        "            pattern_score += 20\n",
        "    \n",
        "    return {\n",
        "        'is_suspicious_pattern': len(suspicious_patterns) > 0,\n",
        "        'suspicious_patterns': suspicious_patterns,\n",
        "        'pattern_score': pattern_score\n",
        "    }\n",
        "\n",
        "print(\"✓ Rule-based functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Rule-Based Filter Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Offensive keywords and abbreviations loaded!\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive offensive keywords list (from rule-based filter)\n",
        "OFFENSIVE_KEYWORDS = [\n",
        "    # Profanity and slurs\n",
        "    'fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt', 'piss',\n",
        "    'crap', 'hell', 'dick', 'pussy', 'cock', 'whore', 'slut', 'fag',\n",
        "    'nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead',\n",
        "    'retard', 'retarded', 'moron', 'idiot', 'stupid', 'dumb', 'fool',\n",
        "    'bullshit', 'crap', 'sucks', 'terrible', 'awful', 'horrible',\n",
        "    \n",
        "    # Extended racism and ethnic slurs\n",
        "    'gook', 'jap', 'slant', 'yellow', 'redskin', 'savage',\n",
        "    'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy',\n",
        "    'house nigger', 'field nigger', 'oreo', 'coconut', 'banana',\n",
        "    'beaner', 'greaser', 'taco', 'burrito',\n",
        "    'sand nigger', 'camel jockey', 'raghead', 'haji',\n",
        "    'slant eye', 'rice eater', 'dog eater',\n",
        "    'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl',\n",
        "    'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig',\n",
        "    'gypsy', 'gyp', 'pikey', 'tinker', 'traveller',\n",
        "    \n",
        "    # Violence and threats\n",
        "    'kill', 'murder', 'death', 'die', 'suicide', 'bomb', 'explode',\n",
        "    'shoot', 'gun', 'weapon', 'knife', 'stab', 'beat', 'hit', 'punch',\n",
        "    'threat', 'threaten', 'harm', 'hurt', 'destroy', 'annihilate',\n",
        "    \n",
        "    # Hate speech\n",
        "    'hate', 'hater', 'racist', 'sexist', 'homophobic', 'transphobic',\n",
        "    'nazi', 'hitler', 'white power', 'black power', 'supremacist',\n",
        "    'genocide', 'ethnic cleansing', 'apartheid', 'segregation',\n",
        "    \n",
        "    # Sexual content\n",
        "    'porn', 'pornography', 'sex', 'sexual', 'nude', 'naked',\n",
        "    'breast', 'boob', 'tit', 'vagina', 'penis', 'dick', 'pussy',\n",
        "    'rape', 'raping', 'molest', 'pedophile', 'pedo', 'incest',\n",
        "    \n",
        "    # Drugs and alcohol\n",
        "    'cocaine', 'heroin', 'marijuana', 'weed', 'cannabis', 'crack',\n",
        "    'meth', 'methamphetamine', 'ecstasy', 'lsd', 'acid', 'mushroom',\n",
        "    'alcohol', 'drunk', 'drinking', 'beer', 'wine', 'vodka',\n",
        "    \n",
        "    # Spam and scams\n",
        "    'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
        "    'casino', 'gambling', 'bet', 'poker', 'slots', 'lottery',\n",
        "    'free money', 'make money', 'earn money', 'quick cash',\n",
        "    'work from home', 'get rich', 'guaranteed', 'no risk',\n",
        "    'click here', 'buy now', 'special offer', 'limited time'\n",
        "]\n",
        "\n",
        "# Offensive abbreviations and internet slang\n",
        "OFFENSIVE_ABBREVIATIONS = {\n",
        "    'wtf': 'what the fuck',\n",
        "    'stfu': 'shut the fuck up',\n",
        "    'gtfo': 'get the fuck out',\n",
        "    'fml': 'fuck my life',\n",
        "    'omfg': 'oh my fucking god',\n",
        "    'lmao': 'laughing my ass off',\n",
        "    'rofl': 'rolling on floor laughing',\n",
        "    'af': 'as fuck',\n",
        "    'btw': 'by the way',\n",
        "    'fyi': 'for your information',\n",
        "    'tbh': 'to be honest',\n",
        "    'imo': 'in my opinion',\n",
        "    'imho': 'in my humble opinion',\n",
        "    'smh': 'shaking my head'\n",
        "}\n",
        "\n",
        "print(\"✅ Offensive keywords and abbreviations loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_offensive_keywords_fixed(text):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Check if text contains any offensive keywords and abbreviations.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains 'is_offensive', 'offensive_words', 'offense_count', and 'offense_score'\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_offensive': False,\n",
        "            'offensive_words': [],\n",
        "            'offense_count': 0,\n",
        "            'offense_score': 0,\n",
        "            'found_abbreviations': []\n",
        "        }\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    found_words = []\n",
        "    found_abbreviations = []\n",
        "    \n",
        "    # Check for exact matches in offensive keywords (word boundaries only) - FIXED PATTERNS\n",
        "    for word in OFFENSIVE_KEYWORDS:\n",
        "        pattern = r'\\\\b' + re.escape(word) + r'\\\\b'\n",
        "        if re.search(pattern, text_lower):\n",
        "            found_words.append(word)\n",
        "    \n",
        "    # Check for offensive abbreviations with sentence context\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for abbrev, full_form in OFFENSIVE_ABBREVIATIONS.items():\n",
        "        if abbrev in text_lower:\n",
        "            # Find the sentence containing the abbreviation\n",
        "            containing_sentence = \"\"\n",
        "            for sentence in sentences:\n",
        "                if abbrev.lower() in sentence.lower():\n",
        "                    containing_sentence = sentence.strip()\n",
        "                    break\n",
        "            \n",
        "            if containing_sentence:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form}) in: '{containing_sentence}'\")\n",
        "            else:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form})\")\n",
        "            found_words.append(abbrev)\n",
        "    \n",
        "    # Calculate offense score based on severity\n",
        "    offense_score = 0\n",
        "    for word in found_words:\n",
        "        if word in ['nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead', 'gook', 'jap', 'slant', 'yellow', 'redskin', 'savage', 'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy', 'house nigger', 'field nigger', 'oreo', 'coconut', 'banana', 'beaner', 'greaser', 'taco', 'burrito', 'sand nigger', 'camel jockey', 'raghead', 'haji', 'slant eye', 'rice eater', 'dog eater', 'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl', 'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig', 'gypsy', 'gyp', 'pikey', 'tinker', 'traveller']:\n",
        "            offense_score += 50  # High severity racial slurs\n",
        "        elif word in ['fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt']:\n",
        "            offense_score += 30  # Medium severity profanity\n",
        "        elif word in ['kill', 'murder', 'death', 'suicide', 'bomb', 'explode']:\n",
        "            offense_score += 40  # High severity violence\n",
        "        elif word in ['hate', 'hater', 'racist', 'sexist', 'homophobic']:\n",
        "            offense_score += 25  # Medium severity hate speech\n",
        "        elif word in ['wtf', 'stfu', 'gtfo', 'fml', 'omfg']:\n",
        "            offense_score += 20  # Offensive abbreviations\n",
        "        else:\n",
        "            offense_score += 10  # Low severity\n",
        "    \n",
        "    return {\n",
        "        'is_offensive': len(found_words) > 0,\n",
        "        'offensive_words': found_words,\n",
        "        'offense_count': len(found_words),\n",
        "        'offense_score': offense_score,\n",
        "        'found_abbreviations': found_abbreviations\n",
        "    }\n",
        "\n",
        "print(\"✅ FIXED Offensive keyword detection function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Offensive keyword detection function created!\n"
          ]
        }
      ],
      "source": [
        "def check_offensive_keywords(text):\n",
        "    \"\"\"\n",
        "    Check if text contains any offensive keywords and abbreviations.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains 'is_offensive', 'offensive_words', 'offense_count', and 'offense_score'\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_offensive': False,\n",
        "            'offensive_words': [],\n",
        "            'offense_count': 0,\n",
        "            'offense_score': 0,\n",
        "            'found_abbreviations': []\n",
        "        }\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    found_words = []\n",
        "    found_abbreviations = []\n",
        "    \n",
        "    # Check for exact matches in offensive keywords (word boundaries only)\n",
        "    for word in OFFENSIVE_KEYWORDS:\n",
        "        pattern = r'\\\\b' + re.escape(word) + r'\\\\b'\n",
        "        if re.search(pattern, text_lower):\n",
        "            found_words.append(word)\n",
        "    \n",
        "    # Check for offensive abbreviations with sentence context\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    for abbrev, full_form in OFFENSIVE_ABBREVIATIONS.items():\n",
        "        if abbrev in text_lower:\n",
        "            # Find the sentence containing the abbreviation\n",
        "            containing_sentence = \"\"\n",
        "            for sentence in sentences:\n",
        "                if abbrev.lower() in sentence.lower():\n",
        "                    containing_sentence = sentence.strip()\n",
        "                    break\n",
        "            \n",
        "            if containing_sentence:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form}) in: '{containing_sentence}'\")\n",
        "            else:\n",
        "                found_abbreviations.append(f\"{abbrev} ({full_form})\")\n",
        "            found_words.append(abbrev)\n",
        "    \n",
        "    # Calculate offense score based on severity\n",
        "    offense_score = 0\n",
        "    for word in found_words:\n",
        "        if word in ['nigger', 'nigga', 'chink', 'kike', 'spic', 'wetback', 'towelhead', 'gook', 'jap', 'slant', 'yellow', 'redskin', 'savage', 'coon', 'jungle bunny', 'porch monkey', 'tar baby', 'mammy', 'house nigger', 'field nigger', 'oreo', 'coconut', 'banana', 'beaner', 'greaser', 'taco', 'burrito', 'sand nigger', 'camel jockey', 'raghead', 'haji', 'slant eye', 'rice eater', 'dog eater', 'heeb', 'yid', 'christ killer', 'jew boy', 'jew girl', 'polack', 'dago', 'wop', 'guinea', 'mick', 'paddy', 'taig', 'gypsy', 'gyp', 'pikey', 'tinker', 'traveller']:\n",
        "            offense_score += 50  # High severity racial slurs\n",
        "        elif word in ['fuck', 'shit', 'damn', 'bitch', 'asshole', 'bastard', 'cunt']:\n",
        "            offense_score += 30  # Medium severity profanity\n",
        "        elif word in ['kill', 'murder', 'death', 'suicide', 'bomb', 'explode']:\n",
        "            offense_score += 40  # High severity violence\n",
        "        elif word in ['hate', 'hater', 'racist', 'sexist', 'homophobic']:\n",
        "            offense_score += 25  # Medium severity hate speech\n",
        "        elif word in ['wtf', 'stfu', 'gtfo', 'fml', 'omfg']:\n",
        "            offense_score += 20  # Offensive abbreviations\n",
        "        else:\n",
        "            offense_score += 10  # Low severity\n",
        "    \n",
        "    return {\n",
        "        'is_offensive': len(found_words) > 0,\n",
        "        'offensive_words': found_words,\n",
        "        'offense_count': len(found_words),\n",
        "        'offense_score': offense_score,\n",
        "        'found_abbreviations': found_abbreviations\n",
        "    }\n",
        "\n",
        "print(\"✅ Offensive keyword detection function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Spam pattern detection function created!\n"
          ]
        }
      ],
      "source": [
        "def detect_spam_patterns(text):\n",
        "    \"\"\"\n",
        "    Detect spam patterns in text using regex patterns.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to check\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains spam detection results and triggered patterns\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'is_spam_pattern': False,\n",
        "            'spam_patterns': [],\n",
        "            'spam_score': 0,\n",
        "            'details': {},\n",
        "            'found_urls': [],\n",
        "            'found_emails': [],\n",
        "            'found_phones': []\n",
        "        }\n",
        "    \n",
        "    text_lower = text.lower()\n",
        "    spam_patterns = []\n",
        "    spam_score = 0\n",
        "    details = {}\n",
        "    found_urls = []\n",
        "    found_emails = []\n",
        "    found_phones = []\n",
        "    \n",
        "    # URL patterns with detailed extraction\n",
        "    url_patterns = [\n",
        "        (r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'URL detected', 30),\n",
        "        (r'www\\\\.[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}', 'WWW URL detected', 25),\n",
        "        (r'[a-zA-Z0-9.-]+\\\\.(com|org|net|edu|gov|mil|int|co|uk|de|fr|jp|au|us|ca|mx|br|es|it|ru|cn|in|kr|nl|se|no|dk|fi|pl|tr|za|th|my|sg|hk|tw|nz|ph|id|vn)', 'Domain detected', 20)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in url_patterns:\n",
        "        matches = re.findall(pattern, text_lower)\n",
        "        if matches:\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "            found_urls.extend(matches)\n",
        "    \n",
        "    # Email patterns with extraction\n",
        "    email_pattern = r'\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b'\n",
        "    email_matches = re.findall(email_pattern, text)\n",
        "    if email_matches:\n",
        "        spam_patterns.append('Email address')\n",
        "        spam_score += 20\n",
        "        details['Email address'] = True\n",
        "        found_emails.extend(email_matches)\n",
        "    \n",
        "    # Phone number patterns with extraction\n",
        "    phone_patterns = [\n",
        "        (r'\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b', 'Phone number (US format)', 25),\n",
        "        (r'\\\\b\\\\d{3}\\\\s?\\\\d{3}\\\\s?\\\\d{4}\\\\b', 'Phone number (spaced format)', 25),\n",
        "        (r'\\\\b\\\\d{3}-\\\\d{3}-\\\\d{4}\\\\b', 'Phone number (dash format)', 25),\n",
        "        (r'\\\\b\\\\d{10}\\\\b', 'Phone number (10 digits)', 20),\n",
        "        (r'\\\\+\\\\d{1,3}\\\\s?\\\\d{1,14}', 'International phone number', 30)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in phone_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "            found_phones.extend(matches)\n",
        "    \n",
        "    # Currency and money patterns\n",
        "    currency_patterns = [\n",
        "        (r'[\\\\$€£¥₹]\\\\s*\\\\d+', 'Currency symbol with number', 15),\n",
        "        (r'\\\\d+\\\\s*[\\\\$€£¥₹]', 'Number with currency symbol', 15),\n",
        "        (r'\\\\d{1,3}(,\\\\d{3})*\\\\s*[\\\\$€£¥₹]', 'Formatted currency', 20),\n",
        "        (r'\\\\$\\\\d+', 'Dollar amount', 10)\n",
        "    ]\n",
        "    \n",
        "    for pattern, description, score in currency_patterns:\n",
        "        if re.search(pattern, text):\n",
        "            spam_patterns.append(description)\n",
        "            spam_score += score\n",
        "            details[description] = True\n",
        "    \n",
        "    # Promotional keywords with specific extraction\n",
        "    promo_keywords = [\n",
        "        'buy now', 'click here', 'free money', 'make money', 'earn money',\n",
        "        'work from home', 'get rich', 'quick cash', 'easy money',\n",
        "        'guaranteed', 'no risk', 'limited time', 'act now', 'dont wait',\n",
        "        'special offer', 'discount', 'sale', 'promotion', 'deal',\n",
        "        'win', 'winner', 'prize', 'lottery', 'jackpot',\n",
        "        'free consultation', 'call now', 'contact us', 'get started'\n",
        "    ]\n",
        "    \n",
        "    found_promo_keywords = []\n",
        "    for keyword in promo_keywords:\n",
        "        if keyword in text_lower:\n",
        "            found_promo_keywords.append(keyword)\n",
        "            spam_score += 10\n",
        "    \n",
        "    if found_promo_keywords:\n",
        "        spam_patterns.append(f'Promotional keywords ({len(found_promo_keywords)})')\n",
        "        details['Promotional keywords'] = found_promo_keywords\n",
        "    \n",
        "    # Medical/pharmaceutical keywords\n",
        "    medical_keywords = [\n",
        "        'viagra', 'cialis', 'pharmacy', 'medication', 'prescription',\n",
        "        'drug', 'pill', 'tablet', 'capsule', 'dosage'\n",
        "    ]\n",
        "    \n",
        "    medical_count = 0\n",
        "    for keyword in medical_keywords:\n",
        "        if keyword in text_lower:\n",
        "            medical_count += 1\n",
        "            spam_score += 15\n",
        "    \n",
        "    if medical_count > 0:\n",
        "        spam_patterns.append(f'Medical keywords ({medical_count})')\n",
        "        details['Medical keywords'] = medical_count\n",
        "    \n",
        "    # Gambling keywords with specific extraction\n",
        "    gambling_keywords = [\n",
        "        'casino', 'gambling', 'bet', 'poker', 'slots', 'lottery',\n",
        "        'jackpot', 'win', 'winner', 'prize', 'money back'\n",
        "    ]\n",
        "    \n",
        "    found_gambling_keywords = []\n",
        "    for keyword in gambling_keywords:\n",
        "        if keyword in text_lower:\n",
        "            found_gambling_keywords.append(keyword)\n",
        "            spam_score += 12\n",
        "    \n",
        "    if found_gambling_keywords:\n",
        "        spam_patterns.append(f'Gambling keywords ({len(found_gambling_keywords)})')\n",
        "        details['Gambling keywords'] = found_gambling_keywords\n",
        "    \n",
        "    return {\n",
        "        'is_spam_pattern': len(spam_patterns) > 0,\n",
        "        'spam_patterns': spam_patterns,\n",
        "        'spam_score': spam_score,\n",
        "        'details': details,\n",
        "        'found_urls': found_urls,\n",
        "        'found_emails': found_emails,\n",
        "        'found_phones': found_phones\n",
        "    }\n",
        "\n",
        "print(\"✅ Spam pattern detection function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Risk Assessment Engine - Hybrid Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Risk assessment engine created!\n"
          ]
        }
      ],
      "source": [
        "def risk_assessment_engine(text):\n",
        "    \"\"\"\n",
        "    Comprehensive risk assessment engine that combines Naive Bayes model with rule-based filter.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        \n",
        "    Returns:\n",
        "        dict: Comprehensive risk assessment with detailed explanations\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'classification': 'safe',\n",
        "            'confidence': 0.0,\n",
        "            'explanation': 'Invalid input - no analysis performed',\n",
        "            'risk_factors': [],\n",
        "            'ml_prediction': None,\n",
        "            'rule_based_analysis': None,\n",
        "            'weighted_score': 0.0\n",
        "        }\n",
        "    \n",
        "    # Step 1: Get ML model prediction (30% weight)\n",
        "    try:\n",
        "        # Preprocess the text (same as training)\n",
        "        processed_text = text.lower().strip()\n",
        "        \n",
        "        # Create TF-IDF features\n",
        "        text_tfidf = vectorizer.transform([processed_text])\n",
        "        \n",
        "        # Create numerical features (simplified for deployment)\n",
        "        # In real deployment, you'd calculate these properly from the text\n",
        "        numerical_features_test = np.zeros((1, len(numerical_features)))\n",
        "        \n",
        "        # Combine features (same as training)\n",
        "        from scipy.sparse import hstack\n",
        "        text_vector = hstack([text_tfidf, numerical_features_test])\n",
        "        \n",
        "        # Make prediction with Naive Bayes model\n",
        "        ml_prediction = naive_bayes_model.predict(text_vector)[0]\n",
        "        ml_probabilities = naive_bayes_model.predict_proba(text_vector)[0]\n",
        "        ml_confidence = max(ml_probabilities)\n",
        "        ml_classification = label_encoder.inverse_transform([ml_prediction])[0].upper()\n",
        "        \n",
        "        # Convert to numeric score (0-1, where 1 is more toxic)\n",
        "        ml_toxic_score = ml_probabilities[1] if len(ml_probabilities) > 1 else 0.0\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ ML model error: {e}\")\n",
        "        ml_toxic_score = 0.0\n",
        "        ml_confidence = 0.0\n",
        "        ml_classification = \"SAFE\"\n",
        "    \n",
        "    # Step 2: Get rule-based analysis (70% weight)\n",
        "    offensive_result = check_offensive_keywords_fixed(text)\n",
        "    spam_result = detect_spam_patterns(text)\n",
        "    \n",
        "    # Calculate rule-based toxic score (0-1)\n",
        "    rule_toxic_score = 0.0\n",
        "    if offensive_result['is_offensive']:\n",
        "        # Convert offense score to 0-1 scale (max offense score is around 200)\n",
        "        # Make it more sensitive to offensive content\n",
        "        rule_toxic_score = min(1.0, offensive_result['offense_score'] / 100.0)\n",
        "    \n",
        "    # Step 3: Spam detection (100% rule-based)\n",
        "    is_spam = spam_result['is_spam_pattern'] and spam_result['spam_score'] >= 20\n",
        "    \n",
        "    # Step 4: Weighted combination for toxic/safe (30% ML + 70% rule-based)\n",
        "    weighted_toxic_score = (0.3 * ml_toxic_score) + (0.7 * rule_toxic_score)\n",
        "    \n",
        "    # Special case: If offensive words are detected, boost the score\n",
        "    if offensive_result['is_offensive'] and offensive_result['offensive_words']:\n",
        "        # Boost the score by at least 0.2 if offensive words are found\n",
        "        weighted_toxic_score = max(weighted_toxic_score, 0.2)\n",
        "    \n",
        "    # Step 5: Final classification logic\n",
        "    if is_spam:\n",
        "        classification = \"spam\"\n",
        "        confidence = min(0.95, 0.6 + (spam_result['spam_score'] / 200.0))\n",
        "        explanation_parts = [\"Spam patterns detected\"]\n",
        "        risk_factors = [f\"Spam score: {spam_result['spam_score']}\"]\n",
        "        \n",
        "        if spam_result['spam_patterns']:\n",
        "            risk_factors.append(f\"Spam patterns: {', '.join(spam_result['spam_patterns'][:3])}\")\n",
        "        if spam_result['found_urls']:\n",
        "            risk_factors.append(f\"URLs found: {', '.join(spam_result['found_urls'][:2])}\")\n",
        "        if spam_result['found_emails']:\n",
        "            risk_factors.append(f\"Emails found: {', '.join(spam_result['found_emails'][:2])}\")\n",
        "        if spam_result['found_phones']:\n",
        "            risk_factors.append(f\"Phone numbers found: {', '.join(spam_result['found_phones'][:2])}\")\n",
        "        if 'Promotional keywords' in spam_result['details']:\n",
        "            promo_keywords = spam_result['details']['Promotional keywords']\n",
        "            if isinstance(promo_keywords, list):\n",
        "                risk_factors.append(f\"Promotional keywords: {', '.join(promo_keywords[:3])}\")\n",
        "        if 'Gambling keywords' in spam_result['details']:\n",
        "            gambling_keywords = spam_result['details']['Gambling keywords']\n",
        "            if isinstance(gambling_keywords, list):\n",
        "                risk_factors.append(f\"Gambling keywords: {', '.join(gambling_keywords[:3])}\")\n",
        "                \n",
        "    elif weighted_toxic_score >= 0.25:\n",
        "        classification = \"toxic\"\n",
        "        confidence = min(0.95, 0.6 + weighted_toxic_score * 0.4)\n",
        "        explanation_parts = [\"High toxicity detected\"]\n",
        "        risk_factors = [f\"Weighted toxic score: {weighted_toxic_score:.3f}\"]\n",
        "        risk_factors.append(f\"ML model prediction: {ml_classification} (confidence: {ml_confidence:.3f})\")\n",
        "        risk_factors.append(f\"ML toxic probability: {ml_toxic_score:.3f}\")\n",
        "        \n",
        "        if rule_toxic_score > 0:\n",
        "            risk_factors.append(f\"Rule-based toxic score: {rule_toxic_score:.3f}\")\n",
        "            if offensive_result['offensive_words']:\n",
        "                risk_factors.append(f\"Offensive words: {', '.join(offensive_result['offensive_words'][:3])}\")\n",
        "            if offensive_result['found_abbreviations']:\n",
        "                risk_factors.append(f\"Offensive abbreviations: {', '.join(offensive_result['found_abbreviations'][:2])}\")\n",
        "                \n",
        "    elif weighted_toxic_score >= 0.15:\n",
        "        classification = \"toxic\"\n",
        "        confidence = min(0.85, 0.5 + weighted_toxic_score * 0.5)\n",
        "        explanation_parts = [\"Moderate toxicity detected\"]\n",
        "        risk_factors = [f\"Weighted toxic score: {weighted_toxic_score:.3f}\"]\n",
        "        risk_factors.append(f\"ML model prediction: {ml_classification} (confidence: {ml_confidence:.3f})\")\n",
        "        risk_factors.append(f\"ML toxic probability: {ml_toxic_score:.3f}\")\n",
        "        \n",
        "        if rule_toxic_score > 0:\n",
        "            risk_factors.append(f\"Rule-based toxic score: {rule_toxic_score:.3f}\")\n",
        "            if offensive_result['offensive_words']:\n",
        "                risk_factors.append(f\"Offensive words: {', '.join(offensive_result['offensive_words'][:2])}\")\n",
        "            if offensive_result['found_abbreviations']:\n",
        "                risk_factors.append(f\"Offensive abbreviations: {', '.join(offensive_result['found_abbreviations'][:1])}\")\n",
        "                \n",
        "    else:\n",
        "        classification = \"safe\"\n",
        "        confidence = min(0.9, 0.7 + (1 - weighted_toxic_score) * 0.3)\n",
        "        explanation_parts = [\"No significant risk factors detected\"]\n",
        "        risk_factors = [f\"Weighted toxic score: {weighted_toxic_score:.3f}\"]\n",
        "        risk_factors.append(f\"ML model prediction: {ml_classification} (confidence: {ml_confidence:.3f})\")\n",
        "        risk_factors.append(f\"ML toxic probability: {ml_toxic_score:.3f}\")\n",
        "        \n",
        "        if rule_toxic_score > 0:\n",
        "            risk_factors.append(f\"Rule-based toxic score: {rule_toxic_score:.3f}\")\n",
        "    \n",
        "    # Create detailed explanation\n",
        "    explanation = f\"Classified as '{classification}' because: {'; '.join(explanation_parts)}. \"\n",
        "    explanation += f\"Risk factors: {'; '.join(risk_factors)}.\"\n",
        "    \n",
        "    return {\n",
        "        'classification': classification,\n",
        "        'confidence': confidence,\n",
        "        'explanation': explanation,\n",
        "        'risk_factors': risk_factors,\n",
        "        'ml_prediction': {\n",
        "            'classification': ml_classification,\n",
        "            'confidence': ml_confidence,\n",
        "            'toxic_probability': ml_toxic_score,\n",
        "            'weight': 0.3\n",
        "        },\n",
        "        'rule_based_analysis': {\n",
        "            'offensive_score': offensive_result['offense_score'],\n",
        "            'spam_score': spam_result['spam_score'],\n",
        "            'toxic_score': rule_toxic_score,\n",
        "            'weight': 0.7\n",
        "        },\n",
        "        'weighted_score': weighted_toxic_score,\n",
        "        'is_spam': is_spam\n",
        "    }\n",
        "\n",
        "print(\"✅ Risk assessment engine created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comprehensive Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing Risk Assessment Engine\n",
            "================================================================================\n",
            "Testing with 24 diverse test cases:\n",
            "================================================================================\n",
            "\n",
            "🔍 Test Case 1\n",
            "------------------------------------------------------------\n",
            "Text: \"This is a great article about machine learning.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.012\n",
            "ML Model: SAFE (confidence: 0.961, toxic prob: 0.039)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.012, ML model prediction: SAFE (confidence: 0.961), ML toxic probability: 0.039\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 2\n",
            "------------------------------------------------------------\n",
            "Text: \"I love this content, it's very informative.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.017\n",
            "ML Model: SAFE (confidence: 0.945, toxic prob: 0.055)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.017, ML model prediction: SAFE (confidence: 0.945), ML toxic probability: 0.055\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 3\n",
            "------------------------------------------------------------\n",
            "Text: \"Thank you for sharing this interesting information.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.006\n",
            "ML Model: SAFE (confidence: 0.979, toxic prob: 0.021)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.006, ML model prediction: SAFE (confidence: 0.979), ML toxic probability: 0.021\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 4\n",
            "------------------------------------------------------------\n",
            "Text: \"Hello, how are you today?\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.027\n",
            "ML Model: SAFE (confidence: 0.908, toxic prob: 0.092)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.027, ML model prediction: SAFE (confidence: 0.908), ML toxic probability: 0.092\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 5\n",
            "------------------------------------------------------------\n",
            "Text: \"You are such an idiot and I hate you!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.712\n",
            "Weighted Score: 0.281\n",
            "ML Model: TOXIC (confidence: 0.936, toxic prob: 0.936)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.281, ML model prediction: TOXIC (confidence: 0.936), ML toxic probability: 0.936\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.281; ML...\n",
            "\n",
            "🔍 Test Case 6\n",
            "------------------------------------------------------------\n",
            "Text: \"This is complete bullshit and you're stupid!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.623\n",
            "Weighted Score: 0.246\n",
            "ML Model: TOXIC (confidence: 0.821, toxic prob: 0.821)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.246, ML model prediction: TOXIC (confidence: 0.821), ML toxic probability: 0.821\n",
            "Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Weighted toxic score: 0.246...\n",
            "\n",
            "🔍 Test Case 7\n",
            "------------------------------------------------------------\n",
            "Text: \"I hate this so much, you're a moron!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.709\n",
            "Weighted Score: 0.273\n",
            "ML Model: TOXIC (confidence: 0.909, toxic prob: 0.909)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.273, ML model prediction: TOXIC (confidence: 0.909), ML toxic probability: 0.909\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.273; ML...\n",
            "\n",
            "🔍 Test Case 8\n",
            "------------------------------------------------------------\n",
            "Text: \"You're a complete asshole, fuck you!\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.064\n",
            "ML Model: SAFE (confidence: 0.788, toxic prob: 0.212)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.064, ML model prediction: SAFE (confidence: 0.788), ML toxic probability: 0.212\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 9\n",
            "------------------------------------------------------------\n",
            "Text: \"WTF is wrong with you? This is stupid!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.764\n",
            "Weighted Score: 0.411\n",
            "ML Model: TOXIC (confidence: 0.902, toxic prob: 0.902)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.411, ML model prediction: TOXIC (confidence: 0.902), ML toxic probability: 0.902\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.411; ML...\n",
            "\n",
            "🔍 Test Case 10\n",
            "------------------------------------------------------------\n",
            "Text: \"STFU and listen, you're an idiot!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.761\n",
            "Weighted Score: 0.402\n",
            "ML Model: TOXIC (confidence: 0.872, toxic prob: 0.872)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.402, ML model prediction: TOXIC (confidence: 0.872), ML toxic probability: 0.872\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.402; ML...\n",
            "\n",
            "🔍 Test Case 11\n",
            "------------------------------------------------------------\n",
            "Text: \"GTFO of here with your bullshit!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.600\n",
            "Weighted Score: 0.200\n",
            "ML Model: SAFE (confidence: 0.849, toxic prob: 0.151)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.200, ML model prediction: SAFE (confidence: 0.849), ML toxic probability: 0.151\n",
            "Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Weighted toxic score: 0.200...\n",
            "\n",
            "🔍 Test Case 12\n",
            "------------------------------------------------------------\n",
            "Text: \"FML, this is terrible and you're making it worse!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.600\n",
            "Weighted Score: 0.200\n",
            "ML Model: SAFE (confidence: 0.852, toxic prob: 0.148)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.200, ML model prediction: SAFE (confidence: 0.852), ML toxic probability: 0.148\n",
            "Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Weighted toxic score: 0.200...\n",
            "\n",
            "🔍 Test Case 13\n",
            "------------------------------------------------------------\n",
            "Text: \"Click here to win $1000! Free money guaranteed!\"\n",
            "Classification: SPAM\n",
            "Confidence: 0.860\n",
            "Weighted Score: 0.012\n",
            "ML Model: SAFE (confidence: 0.959, toxic prob: 0.041)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=52\n",
            "Key Risk Factors: Spam score: 52, Spam patterns: Promotional keywords (4), Gambling keywords (1), Promotional keywords: click here, free money, guaranteed\n",
            "Explanation: Classified as 'spam' because: Spam patterns detected. Risk factors: Spam score: 52; Spam patterns: P...\n",
            "\n",
            "🔍 Test Case 14\n",
            "------------------------------------------------------------\n",
            "Text: \"Buy now! Special offer! Limited time!\"\n",
            "Classification: SPAM\n",
            "Confidence: 0.750\n",
            "Weighted Score: 0.014\n",
            "ML Model: SAFE (confidence: 0.955, toxic prob: 0.045)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=30\n",
            "Key Risk Factors: Spam score: 30, Spam patterns: Promotional keywords (3), Promotional keywords: buy now, limited time, special offer\n",
            "Explanation: Classified as 'spam' because: Spam patterns detected. Risk factors: Spam score: 30; Spam patterns: P...\n",
            "\n",
            "🔍 Test Case 15\n",
            "------------------------------------------------------------\n",
            "Text: \"Visit www.spam-site.com for amazing deals!\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.009\n",
            "ML Model: SAFE (confidence: 0.969, toxic prob: 0.031)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=10\n",
            "Key Risk Factors: Weighted toxic score: 0.009, ML model prediction: SAFE (confidence: 0.969), ML toxic probability: 0.031\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 16\n",
            "------------------------------------------------------------\n",
            "Text: \"Call 555-123-4567 for free consultation!\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.029\n",
            "ML Model: SAFE (confidence: 0.903, toxic prob: 0.097)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=10\n",
            "Key Risk Factors: Weighted toxic score: 0.029, ML model prediction: SAFE (confidence: 0.903), ML toxic probability: 0.097\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 17\n",
            "------------------------------------------------------------\n",
            "Text: \"Email us at spam@example.com for more info!\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.012\n",
            "ML Model: SAFE (confidence: 0.959, toxic prob: 0.041)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.012, ML model prediction: SAFE (confidence: 0.959), ML toxic probability: 0.041\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 18\n",
            "------------------------------------------------------------\n",
            "Text: \"This is not great but not terrible either.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.035\n",
            "ML Model: SAFE (confidence: 0.882, toxic prob: 0.118)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.035, ML model prediction: SAFE (confidence: 0.882), ML toxic probability: 0.118\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 19\n",
            "------------------------------------------------------------\n",
            "Text: \"I'm not sure about this, it seems okay.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.033\n",
            "ML Model: SAFE (confidence: 0.891, toxic prob: 0.109)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.033, ML model prediction: SAFE (confidence: 0.891), ML toxic probability: 0.109\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 20\n",
            "------------------------------------------------------------\n",
            "Text: \"This could be better, but it's acceptable.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.033\n",
            "ML Model: SAFE (confidence: 0.890, toxic prob: 0.110)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=12\n",
            "Key Risk Factors: Weighted toxic score: 0.033, ML model prediction: SAFE (confidence: 0.890), ML toxic probability: 0.110\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 21\n",
            "------------------------------------------------------------\n",
            "Text: \"I hate this\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.082\n",
            "ML Model: SAFE (confidence: 0.728, toxic prob: 0.272)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.082, ML model prediction: SAFE (confidence: 0.728), ML toxic probability: 0.272\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 22\n",
            "------------------------------------------------------------\n",
            "Text: \"hate\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.082\n",
            "ML Model: SAFE (confidence: 0.728, toxic prob: 0.272)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.082, ML model prediction: SAFE (confidence: 0.728), ML toxic probability: 0.272\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 23\n",
            "------------------------------------------------------------\n",
            "Text: \"bullshit\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.045\n",
            "ML Model: SAFE (confidence: 0.849, toxic prob: 0.151)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.045, ML model prediction: SAFE (confidence: 0.849), ML toxic probability: 0.151\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic sco...\n",
            "\n",
            "🔍 Test Case 24\n",
            "------------------------------------------------------------\n",
            "Text: \"wtf\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.739\n",
            "Weighted Score: 0.348\n",
            "ML Model: TOXIC (confidence: 0.694, toxic prob: 0.694)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.348, ML model prediction: TOXIC (confidence: 0.694), ML toxic probability: 0.694\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.348; ML...\n",
            "\n",
            "✅ Risk assessment engine testing completed!\n"
          ]
        }
      ],
      "source": [
        "# Test the risk assessment engine with diverse examples\n",
        "print(\"🧪 Testing Risk Assessment Engine\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Comprehensive test cases\n",
        "test_cases = [\n",
        "    # Safe content\n",
        "    \"This is a great article about machine learning.\",\n",
        "    \"I love this content, it's very informative.\",\n",
        "    \"Thank you for sharing this interesting information.\",\n",
        "    \"Hello, how are you today?\",\n",
        "    \n",
        "    # Toxic content (should be caught by both ML and rule-based)\n",
        "    \"You are such an idiot and I hate you!\",\n",
        "    \"This is complete bullshit and you're stupid!\",\n",
        "    \"I hate this so much, you're a moron!\",\n",
        "    \"You're a complete asshole, fuck you!\",\n",
        "    \n",
        "    # Toxic content with abbreviations\n",
        "    \"WTF is wrong with you? This is stupid!\",\n",
        "    \"STFU and listen, you're an idiot!\",\n",
        "    \"GTFO of here with your bullshit!\",\n",
        "    \"FML, this is terrible and you're making it worse!\",\n",
        "    \n",
        "    # Spam content\n",
        "    \"Click here to win $1000! Free money guaranteed!\",\n",
        "    \"Buy now! Special offer! Limited time!\",\n",
        "    \"Visit www.spam-site.com for amazing deals!\",\n",
        "    \"Call 555-123-4567 for free consultation!\",\n",
        "    \"Email us at spam@example.com for more info!\",\n",
        "    \n",
        "    # Borderline cases\n",
        "    \"This is not great but not terrible either.\",\n",
        "    \"I'm not sure about this, it seems okay.\",\n",
        "    \"This could be better, but it's acceptable.\",\n",
        "    \n",
        "    # Edge cases\n",
        "    \"I hate this\",  # Simple hate\n",
        "    \"hate\",  # Just the word\n",
        "    \"bullshit\",  # Just profanity\n",
        "    \"wtf\",  # Just abbreviation\n",
        "]\n",
        "\n",
        "print(f\"Testing with {len(test_cases)} diverse test cases:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(test_cases, 1):\n",
        "    print(f\"\\n🔍 Test Case {i}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    \n",
        "    # Get risk assessment\n",
        "    result = risk_assessment_engine(text)\n",
        "    \n",
        "    print(f\"Classification: {result['classification'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Weighted Score: {result['weighted_score']:.3f}\")\n",
        "    \n",
        "    # Show ML prediction details\n",
        "    if result['ml_prediction']:\n",
        "        ml = result['ml_prediction']\n",
        "        print(f\"ML Model: {ml['classification']} (confidence: {ml['confidence']:.3f}, toxic prob: {ml['toxic_probability']:.3f})\")\n",
        "    \n",
        "    # Show rule-based analysis\n",
        "    if result['rule_based_analysis']:\n",
        "        rb = result['rule_based_analysis']\n",
        "        print(f\"Rule-based: toxic_score={rb['toxic_score']:.3f}, offensive_score={rb['offensive_score']}, spam_score={rb['spam_score']}\")\n",
        "    \n",
        "    # Show key risk factors\n",
        "    print(f\"Key Risk Factors: {', '.join(result['risk_factors'][:3])}\")\n",
        "    \n",
        "    print(f\"Explanation: {result['explanation'][:100]}{'...' if len(result['explanation']) > 100 else ''}\")\n",
        "\n",
        "print(\"\\n✅ Risk assessment engine testing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Deployment-Ready Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Testing Deployment Function\n",
            "==================================================\n",
            "\n",
            "Test 1: \"This is a normal comment about the weather.\"\n",
            "  Classification: SAFE\n",
            "  Confidence: 0.900\n",
            "  Is Toxic: False\n",
            "  Is Spam: False\n",
            "  Is Safe: True\n",
            "  Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors...\n",
            "\n",
            "Test 2: \"You are such an idiot and I hate you!\"\n",
            "  Classification: TOXIC\n",
            "  Confidence: 0.712\n",
            "  Is Toxic: True\n",
            "  Is Spam: False\n",
            "  Is Safe: False\n",
            "  Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted to...\n",
            "\n",
            "Test 3: \"Click here to win $1000! Free money!\"\n",
            "  Classification: SPAM\n",
            "  Confidence: 0.810\n",
            "  Is Toxic: False\n",
            "  Is Spam: True\n",
            "  Is Safe: False\n",
            "  Explanation: Classified as 'spam' because: Spam patterns detected. Risk factors: Spam score: ...\n",
            "\n",
            "Test 4: \"WTF is wrong with you?\"\n",
            "  Classification: TOXIC\n",
            "  Confidence: 0.726\n",
            "  Is Toxic: True\n",
            "  Is Spam: False\n",
            "  Is Safe: False\n",
            "  Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted to...\n",
            "\n",
            "Test 5: \"I love this content, it's amazing!\"\n",
            "  Classification: SAFE\n",
            "  Confidence: 0.900\n",
            "  Is Toxic: False\n",
            "  Is Spam: False\n",
            "  Is Safe: True\n",
            "  Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors...\n",
            "\n",
            "✅ Deployment function ready!\n",
            "💡 Usage: result = classify_content_for_deployment('your text here')\n"
          ]
        }
      ],
      "source": [
        "def classify_content_for_deployment(text):\n",
        "    \"\"\"\n",
        "    Deployment-ready function for content classification.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Text to classify\n",
        "        \n",
        "    Returns:\n",
        "        dict: Classification result with explanation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = risk_assessment_engine(text)\n",
        "        \n",
        "        # Return simplified result for deployment\n",
        "        return {\n",
        "            'text': text,\n",
        "            'classification': result['classification'],\n",
        "            'confidence': result['confidence'],\n",
        "            'explanation': result['explanation'],\n",
        "            'is_toxic': result['classification'] == 'toxic',\n",
        "            'is_spam': result['classification'] == 'spam',\n",
        "            'is_safe': result['classification'] == 'safe',\n",
        "            'weighted_score': result['weighted_score']\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'text': text,\n",
        "            'classification': 'error',\n",
        "            'confidence': 0.0,\n",
        "            'explanation': f'Error during classification: {str(e)}',\n",
        "            'is_toxic': False,\n",
        "            'is_spam': False,\n",
        "            'is_safe': False,\n",
        "            'weighted_score': 0.0\n",
        "        }\n",
        "\n",
        "# Test the deployment function\n",
        "print(\"🚀 Testing Deployment Function\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "deployment_test_cases = [\n",
        "    \"This is a normal comment about the weather.\",\n",
        "    \"You are such an idiot and I hate you!\",\n",
        "    \"Click here to win $1000! Free money!\",\n",
        "    \"WTF is wrong with you?\",\n",
        "    \"I love this content, it's amazing!\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(deployment_test_cases, 1):\n",
        "    result = classify_content_for_deployment(text)\n",
        "    print(f\"\\nTest {i}: \\\"{text}\\\"\")\n",
        "    print(f\"  Classification: {result['classification'].upper()}\")\n",
        "    print(f\"  Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"  Is Toxic: {result['is_toxic']}\")\n",
        "    print(f\"  Is Spam: {result['is_spam']}\")\n",
        "    print(f\"  Is Safe: {result['is_safe']}\")\n",
        "    print(f\"  Explanation: {result['explanation'][:80]}{'...' if len(result['explanation']) > 80 else ''}\")\n",
        "\n",
        "print(\"\\n✅ Deployment function ready!\")\n",
        "print(\"💡 Usage: result = classify_content_for_deployment('your text here')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing Updated Risk Assessment Engine with Improved Thresholds\n",
            "================================================================================\n",
            "Testing 9 cases with improved thresholds:\n",
            "================================================================================\n",
            "\n",
            "🔍 Test Case 1\n",
            "------------------------------------------------------------\n",
            "Text: \"I hate this\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.082\n",
            "ML Model: SAFE (confidence: 0.728, toxic prob: 0.272)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.082, ML model prediction: SAFE (confidence: 0.728), ML toxic probability: 0.272\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic score: 0.082; ML model ...\n",
            "\n",
            "🔍 Test Case 2\n",
            "------------------------------------------------------------\n",
            "Text: \"hate\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.082\n",
            "ML Model: SAFE (confidence: 0.728, toxic prob: 0.272)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.082, ML model prediction: SAFE (confidence: 0.728), ML toxic probability: 0.272\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic score: 0.082; ML model ...\n",
            "\n",
            "🔍 Test Case 3\n",
            "------------------------------------------------------------\n",
            "Text: \"bullshit\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.045\n",
            "ML Model: SAFE (confidence: 0.849, toxic prob: 0.151)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.045, ML model prediction: SAFE (confidence: 0.849), ML toxic probability: 0.151\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic score: 0.045; ML model ...\n",
            "\n",
            "🔍 Test Case 4\n",
            "------------------------------------------------------------\n",
            "Text: \"wtf\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.739\n",
            "Weighted Score: 0.348\n",
            "ML Model: TOXIC (confidence: 0.694, toxic prob: 0.694)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.348, ML model prediction: TOXIC (confidence: 0.694), ML toxic probability: 0.694\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.348; ML model prediction: T...\n",
            "\n",
            "🔍 Test Case 5\n",
            "------------------------------------------------------------\n",
            "Text: \"You are such an idiot and I hate you!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.712\n",
            "Weighted Score: 0.281\n",
            "ML Model: TOXIC (confidence: 0.936, toxic prob: 0.936)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.281, ML model prediction: TOXIC (confidence: 0.936), ML toxic probability: 0.936\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.281; ML model prediction: T...\n",
            "\n",
            "🔍 Test Case 6\n",
            "------------------------------------------------------------\n",
            "Text: \"This is complete bullshit and you're stupid!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.623\n",
            "Weighted Score: 0.246\n",
            "ML Model: TOXIC (confidence: 0.821, toxic prob: 0.821)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.246, ML model prediction: TOXIC (confidence: 0.821), ML toxic probability: 0.821\n",
            "Explanation: Classified as 'toxic' because: Moderate toxicity detected. Risk factors: Weighted toxic score: 0.246; ML model predictio...\n",
            "\n",
            "🔍 Test Case 7\n",
            "------------------------------------------------------------\n",
            "Text: \"WTF is wrong with you? This is stupid!\"\n",
            "Classification: TOXIC\n",
            "Confidence: 0.764\n",
            "Weighted Score: 0.411\n",
            "ML Model: TOXIC (confidence: 0.902, toxic prob: 0.902)\n",
            "Rule-based: toxic_score=0.200, offensive_score=20, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.411, ML model prediction: TOXIC (confidence: 0.902), ML toxic probability: 0.902\n",
            "Explanation: Classified as 'toxic' because: High toxicity detected. Risk factors: Weighted toxic score: 0.411; ML model prediction: T...\n",
            "\n",
            "🔍 Test Case 8\n",
            "------------------------------------------------------------\n",
            "Text: \"This is a normal comment about the weather.\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.021\n",
            "ML Model: SAFE (confidence: 0.930, toxic prob: 0.070)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.021, ML model prediction: SAFE (confidence: 0.930), ML toxic probability: 0.070\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic score: 0.021; ML model ...\n",
            "\n",
            "🔍 Test Case 9\n",
            "------------------------------------------------------------\n",
            "Text: \"I love this content, it's amazing!\"\n",
            "Classification: SAFE\n",
            "Confidence: 0.900\n",
            "Weighted Score: 0.023\n",
            "ML Model: SAFE (confidence: 0.923, toxic prob: 0.077)\n",
            "Rule-based: toxic_score=0.000, offensive_score=0, spam_score=0\n",
            "Key Risk Factors: Weighted toxic score: 0.023, ML model prediction: SAFE (confidence: 0.923), ML toxic probability: 0.077\n",
            "Explanation: Classified as 'safe' because: No significant risk factors detected. Risk factors: Weighted toxic score: 0.023; ML model ...\n",
            "\n",
            "✅ Updated Risk Assessment Engine tested successfully!\n",
            "💡 Improved thresholds: High toxicity >= 0.25, Moderate toxicity >= 0.15, Safe < 0.15\n",
            "💡 Enhanced rule-based scoring and offensive word detection boost\n"
          ]
        }
      ],
      "source": [
        "# Test the updated Risk Assessment Engine with improved thresholds\n",
        "print(\"🧪 Testing Updated Risk Assessment Engine with Improved Thresholds\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test cases that were previously misclassified\n",
        "problematic_test_cases = [\n",
        "    \"I hate this\",\n",
        "    \"hate\", \n",
        "    \"bullshit\",\n",
        "    \"wtf\",\n",
        "    \"You are such an idiot and I hate you!\",\n",
        "    \"This is complete bullshit and you're stupid!\",\n",
        "    \"WTF is wrong with you? This is stupid!\",\n",
        "    \"This is a normal comment about the weather.\",\n",
        "    \"I love this content, it's amazing!\"\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(problematic_test_cases)} cases with improved thresholds:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(problematic_test_cases, 1):\n",
        "    print(f\"\\n🔍 Test Case {i}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    \n",
        "    # Get risk assessment\n",
        "    result = risk_assessment_engine(text)\n",
        "    \n",
        "    print(f\"Classification: {result['classification'].upper()}\")\n",
        "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "    print(f\"Weighted Score: {result['weighted_score']:.3f}\")\n",
        "    \n",
        "    # Show ML prediction details\n",
        "    if result['ml_prediction']:\n",
        "        ml = result['ml_prediction']\n",
        "        print(f\"ML Model: {ml['classification']} (confidence: {ml['confidence']:.3f}, toxic prob: {ml['toxic_probability']:.3f})\")\n",
        "    \n",
        "    # Show rule-based analysis\n",
        "    if result['rule_based_analysis']:\n",
        "        rb = result['rule_based_analysis']\n",
        "        print(f\"Rule-based: toxic_score={rb['toxic_score']:.3f}, offensive_score={rb['offensive_score']}, spam_score={rb['spam_score']}\")\n",
        "    \n",
        "    # Show key risk factors\n",
        "    print(f\"Key Risk Factors: {', '.join(result['risk_factors'][:3])}\")\n",
        "    print(f\"Explanation: {result['explanation'][:120]}{'...' if len(result['explanation']) > 120 else ''}\")\n",
        "\n",
        "print(\"\\n✅ Updated Risk Assessment Engine tested successfully!\")\n",
        "print(\"💡 Improved thresholds: High toxicity >= 0.25, Moderate toxicity >= 0.15, Safe < 0.15\")\n",
        "print(\"💡 Enhanced rule-based scoring and offensive word detection boost\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary and Integration\n",
        "\n",
        "### 🎯 **Risk Assessment Engine Overview:**\n",
        "\n",
        "The risk assessment engine combines the trained Naive Bayes model with the rule-based filter to provide comprehensive content moderation:\n",
        "\n",
        "#### **Classification Logic:**\n",
        "1. **Spam Detection**: 100% rule-based filter\n",
        "   - Detects URLs, emails, phone numbers, promotional keywords, gambling terms\n",
        "   - Threshold: spam_score >= 20\n",
        "\n",
        "2. **Toxic/Safe Classification**: Weighted combination\n",
        "   - **30% Naive Bayes Model**: Uses `naive_bayes_model.pkl` for ML prediction\n",
        "   - **70% Rule-Based Filter**: Uses offensive keyword detection\n",
        "   - **Thresholds**: \n",
        "     - High toxicity: weighted_score >= 0.25\n",
        "     - Moderate toxicity: weighted_score >= 0.15\n",
        "     - Safe: weighted_score < 0.15\n",
        "\n",
        "#### **Key Features:**\n",
        "- **Detailed Explanations**: Shows specific words, patterns, and scores found\n",
        "- **Confidence Scoring**: Provides confidence levels for each classification\n",
        "- **Risk Factors**: Lists all detected elements (offensive words, spam patterns, etc.)\n",
        "- **Weighted Scoring**: Combines ML and rule-based scores intelligently\n",
        "\n",
        "#### **Deployment Ready:**\n",
        "- `classify_content_for_deployment(text)`: Simple function for production use\n",
        "- Returns: classification, confidence, explanation, boolean flags\n",
        "- Error handling included\n",
        "\n",
        "### 📊 **Integration with Other Components:**\n",
        "- **Naive Bayes Model**: `naive_bayes_model.pkl`, `naive_bayes_vectorizer.pkl`, `naive_bayes_label_encoder.pkl`\n",
        "- **Rule-Based Functions**: Offensive keyword detection, spam pattern detection\n",
        "- **Output**: Compatible with any content moderation system\n",
        "\n",
        "### 🚀 **Usage Examples:**\n",
        "```python\n",
        "# Basic usage\n",
        "result = classify_content_for_deployment(\"Your text here\")\n",
        "print(f\"Classification: {result['classification']}\")\n",
        "print(f\"Confidence: {result['confidence']}\")\n",
        "print(f\"Explanation: {result['explanation']}\")\n",
        "\n",
        "# Check specific types\n",
        "if result['is_toxic']:\n",
        "    print(\"Content is toxic!\")\n",
        "elif result['is_spam']:\n",
        "    print(\"Content is spam!\")\n",
        "else:\n",
        "    print(\"Content is safe!\")\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
